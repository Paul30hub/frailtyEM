\documentclass[nojss]{jss}
%\documentclass[article]{jss}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{natbib}
\usepackage{Sweave}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage[utf8]{inputenc}

%\VignetteIndexEntry{Using frailtyEM for shared frailty models}

%% almost as usual
\author{Theodor Adrian Balan\\Leiden University Medical Center\And
        Hein Putter\\Leiden University Medical Center}
\title{\pkg{frailtyEM}: An \proglang{R} Package for Estimating Semiparametric Shared Frailty Models}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Theodor Adrian Balan, Hein Putter} %% comma-separated
\Plaintitle{frailtyEM: an R package for estimating semiparametric shared frailty models} %% without formatting
\Shorttitle{\pkg{frailtyEM}: An R package for shared frailty models} %% a short title (if necessary)

%% an abstract and keywords

\Abstract{
When analyzing correlated time to event data, shared frailty (random effect) models are particularly attractive. However, the estimation of such models has proved challenging. In semiparametric models, this is further complicated by the presence of the nonparametric baseline hazard.
Although recent years have seen an increased availability of software for fitting frailty models, most software packages focus either on a small number of distributions of the random effect, or support only on a few data scenarios.
\pkg{frailtyEM} is an \proglang{R} package that provides maximum likelihood estimattion of semiparametric shared frailty models using the Expectation-Maximization algorithm. The implementation is consistent across several scenarios, including possibly left truncated clustered failures and recurrent events in both calendar time and gap time formulation. A large number of frailty distributions belonging to the Power Variance Function family are supported. Several methods facilitate access to predicted survival and cumulative hazard curves, both for an individual and on a population level. An extensive number of summary measures and statistical tests are also provided.
}


% \Abstract{
% The software support for fitting ``frailty'' (random effect) models for time-to-event data has grown considerably in the previous years. These models may be used in the context of modeling recurrent event data or clustered failures. The usual problem specific to mixed models, which is integrating over the random effects, is further complicated by the presence of a nonparametric ``baseline'' intensity function. Until recently, the support for such semiparametric models has been limited limited, both in terms of the choice for the random effect distribution and in terms of the type of data that the model can be fitted on. \pkg{frailtyEM} is an \proglang{R} package that estimates shared frailty models using a full maximum likelihood approach based on the Expectation-Maximization algorithm. The software supports a large number of distributions for the random effect from the Power Variance Function family. Left truncated clustered failures and recurrent events in Andersen-Gill or gaptime formulation are also supported, and conditional and marginal estimates of the survival and cumulative hazard are provided.
% }

\Keywords{shared frailty, EM algorithm, recurrent events, clustered failures, left truncation, survival analysis, \proglang{R}}
\Plainkeywords{shared frailty, EM algorithm, recurrent events, clustered failures, left truncation, survival analysis, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}


%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Theodor Adrian Balan\\
  Department of Medical Statistics and Bioinformatics\\
  Leiden University Medical Center\\
  2300 RC Leiden, The Netherlands\\
  E-mail: \email{t.a.balan@lumc.nl}%\\
  %URL: \url{http://eeecon.uibk.ac.at/~zeileis/}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\SweaveOpts{concordance=TRUE}

<<roptions, echo=FALSE>>=
options(prompt="R> ")
options(width=60)
@

\section{Introduction}
Time-to-event data is very common in medical applications. Often, these data are marked by incomplete observations. For example, the phenomena of right censoring occurs when the actual event time is not observed, but the only thing that is known is that the event has not taken place by the end of follow-up. Sometimes, individuals enter the data set only if they have not experienced the event before a certain time point. This is known as left truncation, which, if not accounted for correctly, leads to bias. Regression models for such data have been developed in the field of survival analysis. The most popular is the Cox proportional hazards model \citep{cox1972}, which is semiparametric in nature: the effect of the covariates is assumed to be time-constant and fully parametric, while the time-dependent probability of observing an event arises from the nonparametric baseline hazard.
Cox regression has been the standard in survival analysis for a few reasons. First, it does not require any a priori assumptions about the baseline hazard. Second, under the proportional hazards assumption, maximum likelihood estimation can be carried out efficiently using Cox's partial likelihood. Nowadays, such models may be estimated with most statistical software, such as \proglang{R} \citep{rcitation} \proglang{Stata} \citep{statacitation}, \proglang{SAS} \citep{SAS-STAT} or \proglang{SPSS} \citep{spsscitation}.

When individuals belong to clusters, or may experience recurrent events, the observations are correlated. In this case the Cox model is not appropriate for modeling individual risk. A natural extension is represented by random effect ``shared frailty'' models. Originating from the field of demographics \citep{vaupel1979impact}, these models traditionally assume that the proportional hazards model holds conditional on the frailty, a random effect that acts multiplicatively on the hazard. The variance of the frailty is usually indicative of the degree of heterogeneity in the data. This makes the choice of the random effect distribution relevant. However, the simplicity that made the Cox model so popular does not carry over to such models.

Arguably the most popular way of fitting semiparametric shared frailty models is via the penalized likelihood method \citep{therneaupenalized}, available for the gamma and log-normal frailty distributions. This is the standard in the \pkg{survival} package \citep{survival-book,survival-package} in \proglang{R}, in the \code{PHREG} command in \proglang{SAS} and the \code{streg} procedure in \proglang{Stata}. This method has the advantage that it is generally fast and the Cox model is contained as a limiting case when the variance of the frailty is 0. However, this algorithm can not be used for estimating other frailty distributions or left-truncated data, and the provided standard errors are presented under the assumption that the estimated parameters of the frailty distribution are fixed. Log-normal frailty models may also be estimated in \proglang{R} via Laplace approximation in \pkg{coxme}  \citep{coxme}, h-likelihood in \pkg{frailtyHL} \citep{do2012frailtyhl} or Monte Carlo Expectation-Maximization \pkg{phmm} \cite{phmm1, phmm2, phmm3}. Parametric and spline based shared frailty models are implemented for the gamma and log-normal distributions in the \pkg{frailtypack} package \citep{frailtypack1, frailtypack2}.

In \cite{hougaard2012analysis}, the Power Variance Function (PVF) family was proposed for modeling the frailty distribution. These include the gamma, positive stable (PS), inverse gaussian (IG) and compound Poisson distributions with mass at 0. Each choice of the distribution for the frailty implies a different marginal model, with some emphasizing early depence of the observations (IG) and others the late dependence (gamma). Of particular interest is the PS distribution; for all the others, the hazards are assumed to be proportional conditional on the frailty, but not on the marginal level. For the PS frailty model, the hazards are assumed to be proportional on both levels. Therefore, this is the only distribution where the potential violation of the proportional hazards is not confounded with a frailty effect.

The software implementation of these distributions has so far been limited. At this time, two \proglang{R} packages provide some capabilities: the \pkg{frailtySurv} package \citep{frailtySurv, gorfine2006prospective} implements the above mentioned distributions except the PS via a pseudo full likelihood approach and the \pkg{parfm} package \citep{munda2012parfm} estimates fully parametric gamma, IG, PS and log-normal frailty models.

% semiparametric vs parametric
In this paper we present \pkg{frailtyEM}
\citep{frailtyEM_CRAN}, an \proglang{R} package which uses the general Expectation-Maximization (EM) algorithm \citep{dempster1977maximum} for fitting semiparametric shared frailty models. This implementation comes to complete the landscape of packages that may be used for such models, with support for the whole PVF family of distributions for the scenarios of clustered failures, clusted failures with left truncation and recurrent events data in both calendar time and gap time formulations. Point estimates for regression coefficients are provided with confidence intervals that take into account the estimation of the frailty distribution parameters, and plotting methods facilitate the visualization of both conditional and marginal survival or cumulative hazard curves with 95\% confidence bands, marginal covariate effects, and empirical Bayes estimates of the random effects.
A comparison with respect to functionality between \pkg{frailtyEM} and other \proglang{R} packages is provided in Table~\ref{table1}.

The rest of this paper is structured as follows. In Section~\ref{sec:model} we present a brief overview the semiparametric shared frailty model, and the implications of left truncation. In Section~\ref{sec:Estimation} we discuss the estimation method and its implementation. In Section~\ref{sec:examples} we illustrate the usage of the functions from the \pkg{frailtyEM} package on two classical data sets available in \proglang{R}.

\begin{sidewaystable}[]
\centering
\begin{tabular}{@{}lllllllll@{}}
\toprule
& \pkg{frailtyEM} & \pkg{survival} & \pkg{coxme} & \pkg{frailtySurv} & \pkg{frailtyHL} & \pkg{frailtypack} & \pkg{parfm} & \pkg{phmm} \\ \midrule
\bf Distributions \rm         &           &          &       &             &           &             &       &      \\
Gamma                         & yes       & yes      & no    & yes         & no        & yes         & yes   & no   \\
Log-normal                    & no        & yes      & yes   & yes         & yes       & yes         & yes   & yes  \\
PS                            & yes       & no       & no    & no          & no        & no          & yes   & no   \\
IG                            & yes       & no       & no    & yes         & no        & no          & yes   & no   \\
Compound Poisson              & yes       & no       & no    & no          & no        & no          & no    & no   \\
PVF                           & yes       & no       & no    & yes         & no        & no          & no    & no   \\
\bf Data \rm                  &           &          &       &             &           &             &       &      \\
Clustered failures            & yes       & yes      & yes   & yes         & yes       & yes         & yes   & yes  \\
Recurrent events (AG)         & yes       & yes      & yes   & no          & no        & yes         & no    & no   \\
Left truncation               & yes       & no       & no    & no          & no        & yes         & yes   & no   \\
Correlated structure          & no        & no       & yes   & no          & no        & yes         & no    & yes  \\
\bf Estimation \rm            &           &          &       &             &           &             &       &      \\
Semiparametric               & yes       & yes      & yes   & yes         & yes       & no          & no    & yes  \\
Posterior frailties           & yes       & yes      & no    & no          & no        & yes         & no    & no   \\
Conditional $\Lambda_0,\,S_0$ & yes       & limited  & no    & yes         & no        & yes         & yes   & no   \\
Marginal $\Lambda_0, \, S_0$  & yes       & no       & no    & no          & no        & no          & no    & no   \\ \bottomrule
\end{tabular}
\caption{Comparsion of \proglang{R} packages for frailty models. Versions: \pkg{frailtyEM} 0.7.0, \pkg{survival} 2.40-1, \pkg{coxme} 2.2-5, \pkg{frailtyHL} 1.1, \pkg{frailtypack} 2.10.5, \pkg{parfm} 2.7.1, \pkg{phmm} 0.7-5.}
\label{table1}
\end{sidewaystable}

\section{Model}
\label{sec:model}
We consider the following framework: there are $I$ clusters and $J_i$ individuals in cluster $i$. The outcome from each individual is represented by a realization of a counting process $N_{ij}$. We specify the intensity of $N_{ij}$ as
\begin{equation}
\lambda_{ij}(t|Z_i) = Y_{ij} (t) Z_i \exp(\beta^\top \mathbf{x}_{ij}(t)) \lambda_0(t)
\label{eq:intensity}
\end{equation}
where $Y_{ij}(t)$ indicates whether $N_{ij}$ is under observation at time $t$, $Z_i$ is an unobserved random effect common to all individuals from cluster $i$ (the ``shared frailty''), $\mathbf{x}_{ij}(t)$ a vector of possibly time-dependent covariates, $\beta$ a vector of unknown regression coefficients and $\lambda_0(t) \geq 0$ an unspecified baseline intensity function. We assume that event times are independent given $Z_i = z_i$. We consider the general case where the $Z_i$ follows a distribution with positive support from the infinitely divisible family, i.e., they are i.i.d.~realizations of a random variable described by the Laplace transform
\begin{equation}
\mathcal{L}_Z(c; \alpha, \gamma) \equiv \E\left[ \exp(-Zc)\right] = \exp(-\alpha \psi(c;\gamma))
\label{laplace_transform}
\end{equation}
with $\alpha >0$ and $\gamma > 0$. This formulation includes several distributions, such as the gamma, PS, IG an the general PVF case. These distributions have been extensively studied in \cite{hougaard2012analysis}. Denote $\theta = (\alpha, \gamma)$ as the parameter vector that describes the distribution. The parametrizations used are described in Appendix A1.

\subsection{Likelihood}
The maximum likelihood problem is to maximize the marginal likelihood, based only on the observed data. With the specification \eqref{eq:intensity}, the marginal likelihood is obtained as the product over clusters of expected marginal contributions, i.e.,
\begin{multline*}
L(\theta, \beta, \lambda_0(\cdot)) = \prod_i \E_\theta \left[ \prod_j  \int_0^\infty \left\{Y_{ij}(t) Z_i \exp(\beta^\top \mathbf{x}_{ij}(t) \lambda_0(t) \right\}^{dN_{ij}(t)} \right. \\
\left. \times
\exp\left(-\sum_j\int_0^\infty Y_{ij}(t) Z_i \exp(\beta^\top \mathbf{x}_{ij}(t)) \lambda_0(t) dt\right)
 \right]
\end{multline*}

To make the connection with how this is reflected in the data, we consider that $(i,j,k)$ refers to the \mbox{$k$-th} observation from the \mbox{$j$-th} individual in the \mbox{$i$-th} cluster. Thus, $t_{ijk}$ is the event or censoring time and $\delta_{ijk} = dN_{ij}(t_{ijk})$ is the event indicator for $(i,j,k)$. We write the value of the covariate vector for this observation as $\mathbf{x}_{ijk}$. In the most basic case of clustered failures, $k \equiv 1$, while in the case of recurrent events $j \equiv 1$. More observations for one individual may also arise in the case of clustered failures when the covariates are time-dependent, and the individual is artificially censored at the time when the value of the covariates changes. Nevertheless, the $(i,j,k)$ pair refers to a certain cluster, individual, and a period of time where the covariate vector does not change.

\begin{sloppypar}
The baseline cumulative hazard for observation $(i,j,k)$ is denoted as $\Lambda_{0,ijk}$. Also, let ${\tilde \Lambda_i = \sum_{jk} \exp(\beta^\top \mathbf{x}_{ijk}) \Lambda_{0,ijk}}$. The marginal likelihood can be written as
\end{sloppypar}
$$
L(\theta, \beta, \lambda_0(\cdot)) = \prod_i \E_\theta \left[ \prod_j \left\{\prod_k (Z_i \exp(\beta^\top \mathbf{x}_{ijk})\lambda_0(t_k))^{\delta_{ijk}} \right\} \exp(-z_i \tilde \Lambda_i)\right].
$$
We consider the Breslow estimator for the baseline hazard, i.e., $\lambda_0(t) \equiv \lambda_{0t}$ for $t$ an event time, and 0 otherwise.
%In this case, the direct maximization of the likelihood is very difficult.
By using \eqref{laplace_transform}, the marginal likelihood can be rewritten as
\begin{equation}
L(\theta, \beta, \lambda_0(\cdot)) = \prod_i  \left[ \prod_j \left\{\prod_k (\exp(\beta^\top \mathbf{x}_{ijk})\lambda_0(t_k))^{\delta_{ijk}} \right\} (-1)^{n_i} \mathcal{L}^{(n_i)}_Z(\tilde \Lambda_i)\right],
\label{eq:marginal_likelihood}
\end{equation}
where $\mathcal{L}^{(k)}_Z$ is the $k$-th derivative of the Laplace transform and $n_i$ is the total number of events in cluster $i$.

\subsection{Ascertainment and left truncation}
\label{sec:Ascertainment}
The problem of ascertainment with random effect time-to-event data is usually a difficult one. Consider that the event of observing the cluster $i$ in the data set is $A_i$. Thus, the distribution of the random effect in cluster $i$ is described by the Laplace transform of $Z_i|A_i$, which follows from Bayes' rule as
\begin{equation}
\mathcal{L}_{Z_i|A_i}(c) = \frac{\E\left[\Prob(A_i | Z_i)\exp(-cZ_i) \right]}{\E\left[ \Prob(A_i | Z_i)\right]}.
\label{eq:laplace_conditional}
\end{equation}

Expressing $\Prob(A_i|Z = z)$ depends on the type of the study at hand and on the way the data were collected. In \pkg{frailtyEM} an option is included to deal with the classical scenario of left truncation, i.e., where
$$
\Prob(A_i |Z_i = z_i) = \Prob(T_{i1} > t_{L,i1}, T_{i2} > t_{L,i2} ... T_{J_i} > t_{L,iJ_i} | Z_i = z_i)
$$

Assume that, given $z_i$, the left truncation times $\mathbf{t}_{L,i}$ are independent and the cluster size is not informative. In this case,
\begin{equation}
\Prob(A_i|Z_i = z_i) = \prod_{j=1}^{J_i} \exp\left(-z_i \int_0^{t_{L,ij}} \exp(\beta^\top \mathbf{x}_{ij}(t)) \lambda_0(t) dt \right).
\label{eq:left_trunc1}
\end{equation}
A difficulty here is that the values of the covariate vector and of the baseline intensity must be known prior to the entry time in the study.
%The consequences of the semiparametric model, where $\lambda_0 >0$ only at event time points, are that the $P(T > t) = 1$ for every $t$ before the first event time point.
To assign a value for $\mathbf{x}$ before the entry time is speculative. Therefore, we only consider this case when $\mathbf{x}_i$ is time constant.

With the previous notation, denote the risk accumulated before each of the entry times of cluster $i$ as
$$
\tilde \Lambda_{L,i} = \sum_{j} \exp(\beta^\top \mathbf{x}_{ij}) \Lambda_{0L,ij}
$$
where $\Lambda_{0L,ij}  = \int_0^{t_{L,ij}} \lambda_0(t) dt$. Then, it follows from \eqref{laplace_transform}, \eqref{eq:laplace_conditional} and \eqref{eq:left_trunc1}  that the Laplace transform can be written as
\begin{equation}
\mathcal{L}_{Z|A_i}(c;\alpha,\gamma) = \frac{\exp(-\alpha \psi(c + \tilde\Lambda_{L,i}; \gamma))}{\exp(-\alpha \psi(\tilde\Lambda_{L,i}; \gamma))} = \exp(-\alpha \tilde\psi(c; \tilde\Lambda_{L,i}, \gamma))
\label{eq:laplace_ascertainment}
\end{equation}
where $\tilde\psi(c; \Lambda_{L,i}, \gamma) = \psi(c + \Lambda_{L,i}; \gamma) - \psi(\Lambda_{L,i}; \gamma)$. Thus, the random effect stays in the same infinitely divisible family of distributions under this ascertainment scheme.

Note that, in general, the ascertainment scheme does not have a simple description and $P(A_i | Z_i = z_i)$ may or may not be available in closed form. For example, in family studies, the families may be selected only when a number of individuals live long enough \citep{rodriguez2016survival}. In this case, \eqref{eq:left_trunc1} does not hold. In the case of registry data on recurrent events, individuals (clusters) may be selected only if they have at least one event during a certain time window \citep{balan2016ascertainment}. These specific cases are not currently accommodated by \pkg{frailtyEM}.


\subsection{Goodness of fit and measures of dependence}
\label{sec:goodness}
A reasonable question when fitting random effect models is whether there is evidence for heterogeneity.
To answer this \it a priori \rm, the score test introduced in \cite{commenges1995score} may be used. This test is  referred in \pkg{frailtyEM} as the Commenges-Andersen test, and is performed before the actual maximization of the likelihood, as it does not depend on the frailty distribution and it does not require the actual estimation of the frailty model.

After fitting the model, the likelihood ratio test may be used to assess whether the model with the frailty is a better fit than a model without frailty. In this case, the null model is the model without frailty. With the parametrizations described in Appendix A1, this test lies at the edge of the parameter space, and the test statistic under the null hypothesis follows asymptotically a mixture of $\chi^2(0)$ and $\chi^2(1)$ distribution \citep{lrtstatistic}.

An explicit assumption of model \eqref{eq:intensity} is that the censoring is non-informative on the frailty. This assumption is usually difficult to test. In \pkg{frailtyEM}, a correlation score test is implemented for the gamma distribution, following \cite{balan2016score}. This can also be used, for example, for testing whether a recurrent event event process and a terminal event are associated.

Several measures of dependence are implemented in \pkg{frailtyEM}. The first is the variance of the estimated frailty distribution $Z$, which is useful for the gamma and the PVF family. The variance of $\log Z$ is also useful for the positive stable distribution for which the variance is infinite. Other measures of association include Kendall's $\tau$ and the median concordance. A thorough discussion and comparison of these measures can be found in \cite{hougaard2012analysis}.

\section{Estimation and implementation}
\label{sec:Estimation}
\pkg{frailtyEM} implements a general full-likelihood estimation procedure for the gamma, positive stable and PVF frailty models, based on a profile likelihood method and making use of the expectation-maximization (EM) algorithm \cite{dempster1977maximum}. For fixed parameters of the frailty distribution $\theta$, we define the profile maximum likelihood
\begin{equation}
\widehat L(\theta) = \max_{\beta, \lambda_0} L(\beta, \lambda_0 \vert \theta).
\label{eq:profilelik}
\end{equation}
For each $\theta$, denote $\hat\beta(\theta)$ and $\hat\lambda_0(\theta)$ the value of the parameters that maximize $L(\beta, \lambda_0 \vert \theta)$. A first observation is that, if $\hat\theta$ maximizes $L(\theta)$, then $(\hat \theta, \hat\beta(\hat\theta), \hat\lambda_0(\hat\theta))$ maximize $L(\theta, \beta, \lambda_0)$. Thus, we split the problem of maximizing the likelihood into two: obtaining $\hat\beta(\theta), \hat\lambda_0(\theta)$ for a fixed $\theta$ (the ``inner problem'') and maximizing $L(\theta)$ over $\theta$ (the ``outer problem'').

The \pkg{frailtyEM} package is loaded in the usual way,
<<emfrail0>>==
library("frailtyEM")
@
The main fitting function of the \pkg{frailtyEM} package is \code{emfrail}, with a syntax like:
<<emfrail1, eval=FALSE>>=
emfrail(formula, data, distribution, control, ...)
@
The \code{formula} argument contains a \code{Surv} object on the left hand side and a \code{+cluster()} statement on the right hand side which determines which observations share the same frailty. The \code{Surv} object accepts both \code{(start, status)} for clustered failures and recurrent events in gap time and \code{(start, stop, status)} for clustered failures with left truncation or recurrent events in calendar time.
The \code{distribution} argument determines the distribution of the frailty. It may be generated by the \code{emfrail_dist()} function, with the structure:
<<emfrail2, eval=TRUE>>==
str(emfrail_dist())
@
where \code{dist} may be one of \code{c("gamma", "stable", "pvf")}. For \code{"pvf"}, the \code{m} parameter determines the precise distribution: for $m = -1/2$ for the IG, $m \in \left(-1, 0\right) $ for the so-called Hougaard distribution and $m > 0$ a compound Poisson distribution with mass at 0. The \code{theta} parameter determines the starting value of the optimization and \code{left_truncation} determines whether the \code{start} column, if specified, refers to left truncation. If \code{TRUE}, then the adjustment described in Section \ref{sec:Ascertainment} is performed.

Finally, the \code{control} argument may be generated by the \code{emfrail_control()} function and regulates parameters regarding to the estimation.

Once the user input is checked and processed, the Commenges-Andersen test for heterogeneity is performed and a few calculations regarding the risk sets at different time points are calculated once and cached for use in the maximization.


\subsection{Outer problem}
The ``outer'' problem refers to finding $\widehat \theta$ which maximizes the profile likelihood \eqref{eq:profilelik}. The resulting $\widehat \theta$ is the maximum likelihood estimator and the maximum likelihood is obtained at $\widehat L(\widehat \theta)$. For the infinitely divisible distributions in \pkg{frailtyEM}, $\theta$ is one dimensional. For numerical stability, $\theta$ is introduced on the log-scale in the general purpose maximizer \code{nlm} from the \pkg{stats} package, together with a function that maximizes $L(\beta, \lambda_0 \vert \theta)$. The parameters controling the optimization parameters of \code{nlm} may be passed on from the \code{control} argument.

% Although maximum likelihood estimates are asymptotically normal, the likelihood is likely to be skewed, especially if the maximum likelihood is close to the edge of the parameter space.

\subsection{The inner problem}
For the inner problem of maximizing $L(\beta, \lambda_0 \vert \theta)$ the EM algorithm is used. This has been first proposed for the gamma frailty model in \cite{nielsen1992counting} and \cite{klein1992semiparametric}, and a generalization is discussed in \cite{hougaard2012analysis}.

Most ideas from \cite{nielsen1992counting} are used here. The crucial observation is that the E step involves calculating the empirical Bayes estimates of the frailties $\widehat{z}_i = \E [Z_i | data]$. This expectation is taken with respect to the ``posterior'' distribution of the random effect. Afterwards, the M step is essentially a proportional hazards model with the $\log \widehat{z}_i$ as offset for each cluster.

\paragraph{The E step}
For the E step $\beta$ and $\lambda_0$ are fixed, either at their initial values or at the values from the previous M step. Let $n_i = \sum_{j,k} \delta_{ijk}$ be the number of events in cluster $i$. The conditional distribution of $Z_i$ given the data is described by the Laplace transform
\begin{equation}
\mathcal{L}(c) = \frac{\E\left[ Z_i^{n_i}  \exp(-Z_i \tilde\Lambda_i) \exp(-Z_i c) \right] }{\E\left[ Z_i^{n_i}  \exp(-Z_i \tilde\Lambda_i) \right] } = \frac{\mathcal{L}^{(n_i)}(c + \tilde\Lambda_i)}{\mathcal{L}^{(n_i)}(\tilde\Lambda_i)}.
\label{eq:laplace_transform_estep}
\end{equation}
The E step reduces to calculating the expectation of this distribution, i.e. the derivative of \eqref{eq:laplace_transform_estep} in 0:
\begin{equation}
\widehat{z}_i = -\frac{\mathcal{L}^{(n_i +1)}(\tilde\Lambda_i)}{\mathcal{L}^{(n_i)}(\tilde\Lambda_i)}.
\label{eq:estep}
\end{equation}
The marginal (log-)likelihood is also calculated at this point to keep track of convergence of the EM algorithm. It can be seen that \eqref{eq:marginal_likelihood} involves the denominator of \eqref{eq:laplace_transform_estep} in addition to a straight forward expression of $\beta$ and $\lambda_0$.

The E step is generally the expensive operation of the EM algorithm. In a few scenarios \eqref{eq:estep} may be expressed in a closed form: for the gamma  and the inverse gaussian distributions. In these scenarios, the E step is calculated with the \code{fast_estep()} routine. For all other cases, the E step is calculated via a recursive algorithm with an internal routine which is described in Appendix A2. For easing the computational burden, this is implemented in \proglang{C++} and is interfaced with \proglang{R} via the \pkg{Rcpp} library \citep{rcpp1, rcpp2}.

\paragraph{The M step}
With the same argument as made in \cite{nielsen1992counting}, the M step is equivalent to a regular proportional hazards model with
$\log \widehat{z_i}$ added as an offset for all the cases in $z_i$. This is done via the \code{agreg.fit()} function in the \pkg{survival} package, which estimates of $\beta$ via Cox's partial likelihood. Afterwards, $\lambda_0$ and the subsequent calculations of $\tilde\Lambda_i$  (and, eventually $\tilde \Lambda_{L,i}$, in the case of left truncation) are calculated.

The EM algorithm stops after the marginal log-likelihood has converged, i.e., when difference in $\widehat{L}(\theta)$ is smaller than $\varepsilon$ between two consecutive iterations. The value of $\varepsilon$ can be set with the \code{control} argument of \code{emfrail}.
%
% This procedure takes place in the function \code{em_fit()} which is not directly user accessible (it can be accessed through \code{emfrail()}, by the means of the \code{control} argument).


%  This was suggested in \cite{nielsen1992counting} because this would simplify the EM. Indeed, if $\theta$ is fixed, then the ``complete data'' log-likelihood used in the EM factorizes into
% $$
% \ell = \ell_1(\beta, \lambda_0(\cdot)) + \ell_2(\theta).
% $$
% The nice aspect of treating $\theta$ as fixed is that $\ell_1$ is the log-likelihood of a Cox model and can be maximized with standard software in the M step. Also, for the E step, taking the expectation of $\ell_1$ involves only calculating the $\widehat z = E[\mathbf{z} | data]$, which can be easily obtained from the Laplace transform. The last part,  $\ell_2(\theta)$ is in fact the sum of densities of $z$. This would involve calculating also other expecations except $\widehat z$; furthermore, this densities do not exist in closed form for the positive stable or the PVF distributions.
%
% The main user-accessible function, \code{emfrail()}, creates a matrix representation of the \code{formula} argument which is evaluated in the \code{data.frame} object provided in the \code{data} argument and an internal representation of the frailty distribution given in the \code{distribution} argument. For easier future extensions, the \code{distribution} argument must be an object of the class \code{emfrail_dist} as generated by a call to the \code{emfrail_dist()} function. Several parameters that may be used for controling the precision of the fit or for debugging may be provided in the \code{control} argument, which must be an object of the class \code{emfrail_control} as generated by a call to the \code{emfrail_control()} function.
%
% After obtaining initial values from a Cox model via a call to the \code{agreg.fit()} function of the \pkg{survival} package, the program proceeds to perform an outer and an inner maximization procedure.


% The maximizer used is \code{optimize} from the \pkg{stats} library. This requires the specification of an interval to search for a maximum.

%
% The maximizer of choice may be one of those from the \pkg{optimx} package \citep{optimx1, optimx2}, and it defaults to \code{bobyqa}. The results of this maximization are returned in the final object and are accessible to the user.


% For a fixed $\theta$, the ``complete-data'' log-likelihood is given by
% \begin{equation}
% l_i(\beta, \lambda_0) = \sum_i \sum_j \sum_k \delta_{ijk} ( \log(\lambda_0(t_k)) + \beta' \mathbf{x}_{ijk}) - \sum_{i} z_i  \tilde\Lambda_i ,
% \label{eq:complete_data}
% \end{equation}
% where we ommitted terms that do not involve $\beta$ or $\lambda_0$.
%
% The iterative EM algorithm alternates between two steps; in the E step the conditional expectation of \eqref{eq:complete_data} given the observed data is calculated, and in the M step the resulting expression is maximized with respect to $\beta$ and $\lambda_0$.


\subsection{Standard errors and confidence intervals}
\label{sec:return}
Once the the outer maximization is finished and $\widehat{\log \theta}$ has been obtained, the Hessian is collected from \code{nlm} and, using the delta method as implemented in the \pkg{msm} \citep{msm} package, the variance of $\widehat{\theta}$ is obtained. The 95\% confidence interval for $\widehat{\theta}$ is calculated from a symmetric confidence interval on the log scale, then exponentiated. The resulting asymmetric confidence interval has been seen to provide good coverage \citep{balan2016ascertainment}.

A more precise yet computationally intensive method for quantifying the uncertainty in $\widehat{\log \theta}$ or $\theta$ is through likelihood-based confidence intervals. This requires finding the $\widehat{\theta}$ values for which the difference between the maximum likelihood and the specific profile maximum likelihood values at $\theta$ equals a critical value, calculated from the $\chi^2(1)$ distribution, and is discussed in Appendix A3. This is achieved with the root-finding routine \code{uniroot()} function in the \pkg{stats} package. The major advantage of likelihood-based confidence intervals is that they are invariant to any transformation of the parameter of interest.

The standard error of the estimates for $\beta$ and $\lambda_0(\cdot)$ are calculated with Louis' formula \citep{louis1982finding}, for $\theta$ fixed to the maximum likelihood estimate. The resulting information matrix leads to an underestimate of the standard errors, because it does not account for the uncertainty in estimating $\theta$. These standard errors as provided by the statistical packages that use the penalized likelihood approach, for example. In \pkg{frailtyEM}, adjusted standard errors are also obtained by recalcuating the information matrix for $\beta$ and $\lambda_0$ also at $\hat \theta \pm \varepsilon $. This is described in more detail in Appendix A3. 95\% confidence intervals may be built based on the asymptotic normality of these maximum likelihood estimators.

\subsection{Output, summary and prediction}
The return object type is \code{emfrail}, which is essentially a list that contains the results of the ``outer'' maximization, the results of the ``inner'' maximization at this estimate, and a few other fields which are used for different methods. The object type is documented in \code{?emfrail}.

By itself, the \code{print.emfrail()} method prints the call, a summary of ``outer'' optimization, the estimates of the covariates and the $p$value of the Commenges-Andersen test. A more user-readable summary of an \code{emfrail} object is provided by the \code{summary.emfrail()} method. This returns an object of the class \code{emfrail_summary} that contains general fit information, covariate estimates and several distribution-specific measures of fit and dispersion described in Section~\ref{sec:goodness}. Arguments to \code{summary.emfrail()} may be used to show confidence intervals either likelihood based or delta method based, as described in Section~\ref{sec:return}.

A method for predicting cumulative hazard and survival curves, both conditional and marginal, is implemented in \code{predict.emfrail()}. Confidence bands are based on the asymptotic normality of the estimated $\lambda_0$, and available both for adjusted and un-adjusted for the uncertainty of $\theta$. The user can specify which quantities to obtain for a number of individuals, specified either by a data frame of covariate values or a vector of linear predictor values at which to calculate these curves. With the \code{individual} argument, predicted curves may be produced for individuals with non-specific at-risk patterns (for example, if an individual is not at risk during a certain time frame), or for individuals with time dependent covariates.
The function returns a data frame from which several plots can be easily created.

Two plot methods are provided based on both \pkg{graphics} package via \code{plot.emfrail()} and the \pkg{ggplot2} package, via \code{autoplot.emfrail()}, both with identical syntax. The \code{type} argument may be specified as \code{type = "hist"} for  a histogram of the posterior estimates of the frailties, \code{type = "pred"} for plotting marginal and conditional cumulative hazard or survival curves, and \code{type = "hr"} for plotting marginal and conditional estimated hazard ratios.
Furthermore, a scatter plot of the posterior estimates of the frailties may be obtained from \code{autoplot.emfrail} with \code{type = "frail"}, which also includes quantiles of the posterior distribution in the case of the gamma distribution.

An additional function is provided to calculate the marginal log-likelihood for a vector of values of $\theta$, \code{emfrail_pll()}, without actually performing the outer optimizaion. This may be useful for visualizing the profile log-likelihood or when debugging (e.g., to see if the maximum likelihood estimate of $\theta$ lies on the boundary).

Other methods for \code{emfrail} objects include \code{residuals.emfrail()}, which may be used to obtain martingale residuals, aggregated or individual.
Using the notation of section \ref{sec:model}, if \code{type = "cluster"}, then the vector of $\tilde{\Lambda}_i$ are returned. If \code{type = "individual"}, then for each row in the data $(i,j,k)$ a vector containing
$$
\widehat{z}_i \exp(\widehat{\beta}^\top \mathbf{x}_{ijk}) \Lambda_{0,ijk}
$$
is returned.

\section{Illustration}
\label{sec:examples}
The features of the package will now be illustrated with two well-known data sets available in \proglang{R}.
% Note that \pkg{frailtyEM} package is generally well-suited to work with the \pkg{tidyverse} \citep{tidyverse} tools, such as \pkg{dplyr} \citet{dplyr} and \pkg{ggplot2} \citet{ggplot2}.

\subsection{CGD}
The data are from a placebo controlled trial of gamma interferon in chronic granulotomous disease (CGD) and is available in the \pkg{survival} package. It contains the time to recurrence of serious infections observed, from randomizatio until end of study for each patient.
<<cgd1>>==
data("cgd")
@
For the purpose of illustration, we will use \code{treat} (treatment or placebo) and \code{sex} (female or male) as covariates, althought a larger number of variables are recorded in the data set.

A basic gamma frailty model can be fitted like this:
<<cgd2>>==
gam <- emfrail(Surv(tstart, tstop, status) ~ sex + treat + cluster(id),
  data = cgd)
summary(gam, lik_ci = TRUE)
@

The first two parts of this output, about regression coefficients and fit summary, exist regardless of the frailty distributions. The last part, ``frailty summary'', provides a useful output according to the distribution. The calculations behind this part are described for each distribution in Appendix A1. Since only $\log \theta$ is actually estimated in the ``outer'' step, the delta method is employed to obtain standard errors for all derived quantities. The confidence intervals may be obtained either likelihood-based or delta method-based, see Appendix A3 for details. The delta method based confidence intervals are shown with the option \code{lik_ci = FALSE}.

Both the Commenges-Andersen test for heterogeneity and the one-sided likelihood ratio test deems the random effect highly significant. This is also suggested by the confidence interval for the frailty variance, which is far from 0.

To illustrate the predicted cumulative hazard curves we take two individuals, one from the treatment arm and one from the placebo arm, both males:
<<bladder2_cumhaz>>==
library("ggplot2")
p1 <- autoplot(gam, type = "pred",
  newdata = data.frame(sex = "male", treat = "rIFN-g")) +
  ggtitle("rIFN-g") + ylim(c(0, 2)) + theme_minimal()

p2 <- autoplot(gam, type = "pred",
  newdata = data.frame(sex = "male", treat = "placebo")) +
  ggtitle("placebo") + ylim(c(0, 2)) + theme_minimal()
@
The two plots are shown in Figure~\ref{fig:plot_pred}.
\begin{figure}
\begin{center}
<<cgd_pred_plots, include=FALSE,echo=FALSE,results=hide>>=
pdf("cgd_pred.pdf", width = 9, height = 3)
gridExtra::grid.arrange(p1, p2, nrow = 1)
dev.off()
@
\centering\includegraphics[width = 15.5cm, height = 5.16cm]{cgd_pred}
\caption{Predicted conditional and marginal cumulative hazards for males, one from the treatment arm and one from the placebo arm, as produced by \code{autplot.emfrail()} with \code{type = "pred"}. }
\label{fig:plot_pred}
\end{center}
\end{figure}

The cumulative hazard in this case can be interpreted as the expected number of events at a certain time. It can be seen that the frailty ``drags down'' the marginal hazard. This is a well-known effect observed in frailty models, as described in \citet[ch.~7]{aalen2008survival}.

If \code{autoplot.emfrail()} is called with \code{type = "pred"}, this will also call \code{predict.emfrail()}. All prediction results could also be obtained directly:
<<pred1, eval=FALSE>>==
dat_pred <- data.frame(sex = c("male", "male"),
  treat = c("rIFN-g", "placebo"))
predict(gam, dat_pred)
@
For a hypothetical individual that changes treatment from placebo to \mbox{rIFN-g} at time 200, predictions may also be obtained:
<<pred_timechange>>==
dat_pred_b <- data.frame(sex = c("male", "male"),
  treat = c("placebo", "rIFN-g"),
  tstart = c(0, 200), tstop = c(200, Inf))

p <- autoplot(gam, type = "pred", newdata = dat_pred_b, individual = TRUE) +
  ggtitle("change placebo to rIFN-g at time 200") + theme_minimal()
@
This plot is shown in Figure~\ref{fig:predchange}.

\begin{figure}
\begin{center}
<<cgd_pred_change,  include=FALSE,echo=FALSE,results=hide>>=
pdf("cgd_treatdif.pdf", width = 5, height = 4)
p
dev.off()
@
\includegraphics[width = 10cm, height = 8cm]{cgd_treatdif}
\caption{Predicted conditional and marginal cumulative hazards for a male that switches treatment from placebo to \mbox{rIFN-g} at time 200 as produced by \code{autoplot.emfrail()} with \code{type = "pred"}}
\label{fig:predchange}
\end{center}
\end{figure}

A positive stable frailty model can also be fitted by specifying the \code{distribution} argument.
<<bladder2_stable>>==
stab <- emfrail(Surv(tstart, tstop, status) ~ sex + treat + cluster(id),
  data = cgd,
  distribution = emfrail_dist(dist = "stable"))
summary(stab)
@

The coefficient estimates are similar to those of the gamma frailty fit. The ``Frailty summary'' part is quite different though. For the positive stable distribution, the variance is not defined. However, Kendall's $\tau$ is easily obtained, and in this case it is smaller than in the gamma frailty model. Unlike the gamma or PVF distributions, the positive stable frailty predicts a marginal model with proportional hazards where the marginal hazard ratios are an attenuated version of the conditional hazard ratios shown in the output. The calculations are detailed in Appendix A1.

The conditional and marginal hazard betw ratios from different distributions can also be visualized easily.  We also fitted an IG frailty model on the same data, and plots of the hazard ratio between two males from different treatment arms created below are shown in Figure~\ref{fig:hr}.
<<bladder2_hazardratios>>==
ig <- emfrail(Surv(tstart, tstop, status) ~ sex + treat + cluster(id),
  data = cgd,
  distribution = emfrail_dist(dist = "pvf"))

newdata <- data.frame(treat = c("placebo", "rIFN-g"),
  sex = c("male", "male"))

pl1 <- autoplot(gam, type = "hr", newdata = newdata) +
  ggtitle("gamma") + theme_minimal()

pl2 <- autoplot(stab, type = "hr", newdata = newdata) +
  ggtitle("PS") + theme_minimal()

pl3 <- autoplot(ig, type = "hr", newdata = newdata) +
  ggtitle("IG") + theme_minimal()
@
While all models shrink the hazard ratio towards 1, it can be seen that this effect is slightly more pronounced for the gamma than for the IG, while the PS exhibits a constant ``average'' shrinkage. This type of behaviour from the PS is seen as a strength of the model \citep{hougaard2012analysis}.


\begin{figure}
\begin{center}
<<cgd_hr_plots,  include=FALSE,echo=FALSE,results=hide>>=
pdf("cgd_hr.pdf", width = 9, height = 3)
gridExtra::grid.arrange(pl1, pl2, pl3, nrow = 1)
dev.off()
@
\includegraphics[width = 15.5cm, height = 5.16cm]{cgd_hr}
\caption{Conditional and marginal hazard ratio between two males from the placebo and \mbox{rIFN-g}  treatment arms  from the gamma, PS and IG frailty models as produced by \code{autoplot.emfrail()} with \code{type = "hr"}.}
\label{fig:hr}
\end{center}
\end{figure}


\subsection{Kidney}
The \code{kidney} data set is also available in the \pkg{survival} package.
%
% In an experiment, each of M individuals is observed and times between recurrences of a particular type of event are recorded. The problem that motivates this study is the recurrence of infection in kidney patients who are using a portable dialysis machine. The infection occurs at the point of insertion of the catheter and, when it occurs, the catheter must be removed, the infection cleared up, and then the catheter reinserted. Recurrence times are times from insertion until the next infection. Sometimes the catheter must be removed for other reasons so that there may be right censoring of the data. As well, the final recurrence time may be censored. It is assumed here that each patient is followed for a predetermined number of recurrence times, some of which may be censored.
The data, presented originally in \cite{mcgilchrist1991regression}, contains the time to infection for kidney patients using  a portable dialysis equipment. The infection may occur at the insertion of the catheter and at that point, the catheter must be removed, the infection cleared up, and the catheter reinserted. Each of the 38 patients has exactly 2 observations, representing recurrence times from insertion until the next infection. There are 3 covariates: sex, age and disease (a factor with 4 levels). A data analysis based on frailty models is described in \citet[ch.~9.5.2]{survival-book}. The authors note that, when \code{disease} is included in the model, a gamma frailty model  offers no evidence of heterogeneity. However, when \code{disease} is removed from the model, then there seems to be moderate evidence for heterogeneity. This is an example where the frailty may be interpreted as a missing covariate.
<<kidney1>>==
data("kidney")
kidney$sex <- ifelse(kidney$sex == 1, "male", "female")

m_gam <- emfrail(Surv(time, status) ~ age + sex + cluster(id),
  data = kidney)
s_gam <- summary(m_gam)
s_gam
@

Therneau and Grambsch discuss these models and they conclude that an outlier case is at the source of the frailty effect. With the \pkg{frailtyEM} package, the positive stable frailty model may also be fitted. Unlike the gamma frailty model, the positive stable does not attempt to ``correct'' non-proportional hazards.

<<kidney1>>==
m_stab <- emfrail(Surv(time, status) ~ age + sex + cluster(id),
  data = kidney,
  distribution = emfrail_dist(dist = "stable"))
s_stab <- summary(m_stab)
s_stab
@

The Commenges-Andersen test for heterogeneity shows the same evidence as before, as it does not depend on the frailty distribution. However, the positive stable parameter lies at the edge of the parameter space ($\theta$ is between 0 and 1 for the PS distribution). Therefore, the LRT is not significant. The major difference with the gamma frailty fit is that the regression coefficient for sex is much smaller. To untangle this effect, one can check the (marginal) proportional hazards assumption.

A test for proportionality, based on Schoenfeld residuals, is implemented in the \pkg{survival} package. This reveals that \code{sex} has a significantly non-proportional effect on the marginal hazards:
<<kidney3>>==
zph1 <- cox.zph(coxph(Surv(time, status) ~ age + sex + cluster(id),
  data = kidney))
zph1
@

The same test may be performed with two other models, by including the estimated frailties as offset. This would be a test for proportionality, conditional on the frailties.
<<kidney4>>==
off_z_gam <- log(s_gam$frail$z)[match(kidney$id, s_gam$frail$id)]
off_z_stab <- log(s_stab$frail$z)[match(kidney$id, s_stab$frail$id)]

zph_gam <- cox.zph(coxph(Surv(time, status) ~
  age + sex + offset(off_z_gam) + cluster(id),
  data = kidney))
zph_stab <- cox.zph(coxph(Surv(time, status) ~
  age + sex + offset(off_z_stab) + cluster(id),
  data = kidney))
zph_gam
zph_stab
@

In this case, it can be seen that the gamma frailty corrects for proportionality while the PS distribution does not, indicating that there might be that the gamma frailty might pick up the marginal non-proportionality rather than heterogeneity.
%
% \subsection{kidney data set}
% The \code{kidney} data set is available in the \pkg{survival} package.
% <<kidney>>==
% data(kidney)
% kidney$sex <- kidney$sex -1
% head(kidney)
% @
% The number of events and observations for each individual:
% <<kidney2>>==
% library(tidyverse)
% kidney %>%
%   group_by(id) %>%
%   summarize(nobs = n(),
%             nev = sum(status)) %>%
%   gather(key = variable, value = number, nobs, nev) %>%
%   ggplot(aes(x = number)) + geom_bar() + facet_grid(~variable)
%
% @
% Gamma frailty
% <<kidney3>>==
% m_ft <- emfrail(data = kidney,
%                 formula = Surv(time, status) ~ age + sex + cluster(id))
% summary(m_ft)
%
% m_cph <- coxph(Surv(time, status) ~ age + sex + frailty(id), data = kidney, ties = "breslow")
% summary(m_cph)
% @

\section{Conclusion}
In the current landscape for modeling random effects in survival analysis, \pkg{frailtyEM} is a contribution that focuses on implementing classical methodology in an efficient way. We have shown that the EM based approach has certain advantages in the context of frailty models. First of all, it is semiparametric, which means that it is a direct extension of the Cox proportional hazards model. In this way, classical results from semiparametric frailty models (for example, based on the data sets in Section~\ref{sec:examples}) can be replicated and further insight may be obtained by fitting models with different frailty distributions. Until now, the Commenges-Andersen test, positive stable and PVF family, have not all been implemented in a consistent way in an \proglang{R} package. Another advantage of the EM algorithm is that, by its nature, it is a full maximum likelihood approach, and the estimators have well known desirable asymptotic properties.

To our knowledge, no other statistical package provides similar capabilities for visualizing conditional and marginal survival curves, or the marginal effect of covariates. Since this is implemented across a large number of distributions, this might come to the aid of both applied and theoretical research into shared frailty models. While the question of model selection with different random effect distributions is still an open one, the functions included \pkg{frailtyEM} may be useful for further research in this direction.

Evaluating goodness of fit for shared frailty models is still a complicated issue, particularly in semiparametric models. However, tests based on martingale residuals, such as that of \cite{commenges2000standardized}, should be now possible by extrating the necessary quantities from an \code{emfrail} fit.

In this paper we have not exemplified the left truncation adjustment. In this respect, the implementation from \pkg{frailtyEM} is very similar to that from the \pkg{parfm} package. However, performing of a larger simulation study to assess the effects of left truncation in clustered failure data with semiparametric frailty models is now possible. The scenario of time dependent covariates and left truncation is not supported at this time. This is because this would require also specifying values of these covariates from time 0 to the left truncation time, which would probably involve some speculation.

Technically, extending the package to other distributions is possible, as long as their Laplace transform and the corresponding derivatives may be specified in closed form. An interesting extension would be to choose discrete distributions from the infinitely divisible family for the random effect, such as the Poisson distribution.
The newest features will be implemented in the development version of the package at \url{https://github.com/tbalan/frailtyEM}.


%
%
% %% include your article here, just as usual
% %% Note that you should use the \pkg{}, \proglang{} and \code{} commands.
%
% %\section[About Java]{About \proglang{Java}}
% %% Note: If there is markup in \(sub)section, then it has to be escape as above.
%
%
\section*{Appendix A1: Results for the Laplace transforms}
We consider distributions from the infinitely divisible family \cite[ch 8.5]{ash2014real} with the Laplace transform
$$
\mathcal{L}_Y(c) = \exp(-\alpha \psi(c;\gamma)).
$$
We now consider how $\alpha$ and $\gamma$ can be represented as a function of a positive parameter $\theta$.

\paragraph{The gamma distribution}
For $Y$ a gamma distributed random variable, $\psi(c;\gamma) = \log(\gamma + c) - \log(\gamma)$, the derivatives of which are
$$\psi^{(k)} (c; \gamma) = (-1)^{k-1} (k-1)! (\gamma + c)^{-k}.$$
For identifiability, the restriction $\E Y = 1$ is imposed; this leads to $\alpha = \gamma$.
The distribution is parametrized with $\theta >0$, $\theta = \alpha = \gamma$.
The variance of $Y$ is  $\VAR Y = \theta^{-1}$.
Kendall's $\tau$ is then $\tau = \frac{1}{1 + 2 \theta}$ and the median concordance is
$\kappa = 4 \left( 2^{1 + 1 / \theta} - 1 \right)^{-\theta} - 1$. Furthermore,
$\E \log Y = \psi(\theta) - \log \theta$ and
$\VAR \log Y = \psi'(\theta)$ where $\psi$ and $\psi'$ are the digamma and trigamma functions.

\paragraph{The positive stable distribution}
For $Y$ a positive stable random variable,
$\psi(c;\gamma) = c^\gamma$ with $\gamma \in (0,1)$, the derivatives of which are
$$
\psi^{(k)}(c; \gamma) = \frac{\Gamma(k - \beta)}{\Gamma(1 - \gamma)} (-1)^{k - 1} c^{\gamma - k}.
$$
For identifiability, the restriction $\alpha = 1$ is made; $\E Y$ is undefined and $\VAR Y = \infty$.
The distribution is parametrized with  $\theta >0$, $\gamma = \frac{\theta}{\theta + 1}$.

% This parametrization is equivalent to that of \cite{munda2012parfm}. In \cite{hougaard2012analysis}, the parameter
% $\alpha_{H} = \gamma$.
Kendall's $\tau$ is then
$\tau = 1 - \frac{\theta}{\theta + 1}$ and the median concordance is
$\kappa = 2^{2 - 2^{\frac{\theta}{\theta + 1}}} - 1$.
Furthermore, $\E \log Y = - \left(\left\{\frac{\theta}{1 + \theta}\right\}^{-1} - 1\right) \psi(1)$ and
$\VAR \log Y = \left( \left\{\frac{\theta}{1 + \theta} \right\}^{-2} - 1 \right) \psi'(1)$.

In the case of the PS distribution, the marginal hazard ratio is an attenuated version of the conditional hazard ratio. If the conditional log-hazard ratio is $\beta$, the marginal hazard ratio  is equal to $\beta \frac{\theta}{\theta + 1}$.


\paragraph{The PVF distributions}
For $Y$ a PVF distribution with fixed parameter $m \in \mathbb{R}$, $m > -1$ and $m \neq 0$,
$$
\psi(c; \gamma) = (1 - \gamma^m (\gamma + c)^{-m}).
$$
This is the same parametrizaion as in \cite{aalen2008survival}.
The derivatives of $\psi$ are
$$
\psi^{(k)}(c; \gamma) =  (-\gamma)^m (\gamma + c)^{-m-k} (-1)^{k+1} \frac{\Gamma(m + k)}{\Gamma(m)}.
$$
The expectation of this distribution can be calculated as minus the first derivative of the Laplace transform calculated in 0, i.e.,
$$
\E Y = \alpha \psi'(0;\gamma) \mathcal{L}(0;\alpha, \gamma) = \frac{\alpha}{\gamma} m.
$$
The second moment of the distribution can be calculated as the second derivative of the Laplace transform at 0,
$$
\E Y^2 = \alpha^2 \psi'^2(0) -\alpha \psi''(0) = \frac{\alpha^2}{\gamma^2}m^2 + \frac{\alpha}{\gamma^2} m(m+1).
$$
For identifiability, we set $\E Y = 1$. The distribution is parametrized through a parameter $\theta >0$ which is determined by
$\gamma = (m + 1) \theta$ and
$\alpha = \frac{m + 1}{m} \theta$.
This results in $\VAR Y = \theta^{-1}$.

A slightly different parametrization is presented in \cite{hougaard2012analysis}, dependent on the parameter $\eta_H$. The correspondence is obtained by setting $\eta_H = (m+1) \theta$.

The PVF family of distributions includes the gamma as a limiting case when $m \rightarrow 0$. When $\gamma \rightarrow 0$ the positive stable distribution is obtained. When $m = -1$ the distribution is degenerate, and with $m = 1$ a non-central gamma distribution is obtained. Of special interest is the case $ m = -0.5$, when the inverse Gaussian distribution is obtained.  With $m >0$, the distribution is compound Poisson with mass at 0. In this case, $P(Y = 0) = \exp(- \frac{m+1}{m} \theta)$.

For $m < 0$, closed forms for Kendall's $\tau$ and median concordance are given in \citet[Section 7.5]{hougaard2012analysis}.
% For the inverse Gaussian, also $\E \log Y$ and $\VAR \log Y$ are available.


\subsection*{Left truncation}
To determine the Laplace transform under left truncation, we determine $\tilde \psi$ from \eqref{eq:laplace_ascertainment}.

For the gamma distribution, we have
$$
\tilde \psi (c;\gamma, \Lambda_L) = \log(\gamma + \Lambda_L + c) - \log(\gamma + \Lambda_L)
$$
which implies that the frailty of the survivors is still gamma distributed, but with a change in the parameter $\gamma$.

For the positive stable we have
$$
\tilde \psi(c; \gamma, \Lambda_L) = (c+\Lambda_L)^{\gamma} - \Lambda_L^\gamma,
$$
which is not a positive stable distribution any more.

For the PVF distributions, we have
$$
\tilde \psi(c; \gamma, \Lambda_L) = \mathrm{sign}(m) \left( \gamma^m(\gamma + \Lambda_L)^{-m} - (\gamma + \Lambda_L)^m (\gamma + \Lambda_L + c)^{-m} \right),
$$
which is not PVF any more.

\subsection*{Closed forms}
The gamma distribution leads to a Laplace transform for which the derivatives can be calculated in closed form. It can be seen that
$$
\mathcal{L}(c;\alpha, \gamma) = \gamma^\alpha (\gamma + c)^{-\alpha}.
$$
The $k$-th derivative of this expression is
$$
\mathcal{L}^{(k)}(c;\alpha, \gamma) = \gamma^\alpha (\gamma + c)^{-\gamma - k} \frac{\Gamma(\alpha + k)}{ \Gamma(\alpha)} .
$$
This can be exploited also in the case of left truncation, since the gamma frailty is preserved, as shown in the previous section.

The inverse gaussian distribution is obtained when the PVF parameter is $m=-\frac{1}{2}$. Under the current parametrization, we have $\gamma = \theta / 2$ and $\alpha = \theta$. In this case, the Laplace transform is
$$
\mathcal{L}(c; \theta) = \exp \left\{\theta \left(1 - \sqrt{1 + 2c/\theta }\right) \right\}.
$$
The $k$-th derivative of this can be written as
$$
\mathcal{L}^{(k)}(c; \theta) = (-1)^k \left(\frac{2}{\theta} c + 1\right)^{-k/2} \frac{\mathcal{K}_{k - 1/2} \left(\sqrt{ 2\theta \left(c + \frac{\theta}{2}\right)}\right)}{\mathcal{K}_{1/2} \left( \sqrt{ 2\theta \left(c + \frac{\theta}{2}\right)}\right)}
$$
where $\mathcal{K}$ is the modified Bessel function of the second kind.

The \code{emfrail()} uses the closed form formulas when possible, by default.

\section*{Appendix A2: A general E step}
As shown in \eqref{eq:laplace_transform_estep}, the calculation of the E step for the general case involves taking derivatives of Laplace transforms of the form
$$
\mathcal{L}(c) = \exp(g(c))
$$
where for simplicity we denote $g(c) = -\alpha \psi(c;\gamma)$.
The expression for the $k$-th derivative of $\mathcal{L}(c)$ can be obtained with a classical calculus result, di Bruno's formula, i.e.,
\begin{equation}
\mathcal{L}^{(n)}(c) = \sum_{\mathbf{m} \in \mathcal{M}_n}\frac{n!}{m_1! m_2! ... m_n!} \prod_{j=1}^n \left( \frac{g^{(j)}(c)}{j!} \right)^{m_j} \mathcal{L}(c),
\label{eq:dibruno}
\end{equation}
where $\mathcal{M}_n = \left\{ (m_1, ..., m_n)\right | \sum_{j=1}^n j \times m_j = n \}$.
For example, for $n = 3$,
$$
\mathcal{M}_3 = \left\{ (3, 0, 0), \, (1, 1, 0),\, (0, 0, 1) \right\}.
$$
This corresponds to the ``partitions of the integer'' 3, i.e., all the integers that sum up to 3:
$$
\left\{(1, 1, 1), \, (1, 2, 0), (3,0,0)\right\}.
$$
We implemented a recursive algorithm in \proglang{C++} which resides in the \code{emfrail_estep.cpp} which loops through these partitions, calculates the corresponding derivatives of $\psi$ and the coefficients.

\section*{Appendix A3: Standard errors}
The outer maximization of $\widehat{L}(\theta)$ is carried out on the log-scale, as described in section~\ref{sec:Estimation}, and the numeric hessian is used to obtain $\VAR (\widehat\theta)$.
Afterwards, the delta method is employed to derive standard errors for $\theta$ and the other functionals of $\theta$ described in Appendix A1. However, the standard error is not very meaningul for parameters with skewed distributions. Confidence intervals are constructed in two ways.

The first type of confidence intervals provided by \pkg{frailtyEM} are based on the the asymptotic normality of $\widehat{\log\theta}$, by constructing a 95\% symmetric confidence interval on the log-scale, and then translating it to the other functionals of $\theta$.

The second type are likelihood-based confidence intervals. Under the null hypothesis, the likelihood ratio test statistic follows a $\chi^2(0) + \chi^2(1)$ distribution. The critical value associated with this test statistic is approximately $1.92$. Using the root-finding algorithm implemented in the \code{uniroot()} function in the \pkg{stats} package, a confidence interval is obtained from the values of $\theta$ with the property that $\widehat{L}(\theta) \geq \widehat{L}(\widehat{\theta}) - 1.92$. This confidence interval is then translated to the functionals of $\theta$.

The likelihood-based confidence intervals are the default in \code{emfrail()} because the coverage is guaranteed to be the same for all transformations of $\theta$.

Considering the vector of parameters $\eta = (\beta, \lambda_0(\cdot))$, the information matrix for $(\theta, \eta)$ can be written as follows:
\[
\mathcal{I}=
  \begin{bmatrix}
    \mathcal{I}_{\theta, \theta} & \mathcal{I}_{\theta, \eta}  \\
    \mathcal{I}_{\eta, \theta} & \mathcal{I}_{\eta, \eta}
  \end{bmatrix}.
\]
 The part corresponding to $\eta$, $\mathcal{I}_{\eta,\eta}$ is calculated using Louis' formula, which has been commonly employed to obtain this quantity from EM algorithms \cite{louis1982finding}. This is done under the assumption of $\theta$ fixed to the maximum likelihood estimate $\hat\theta$. This leads to an underestimate of the standard errors, as is noted also in \citet[sec. 9.5]{survival-book}.
 The calculation of the variance-covariance matrix $\mathcal{I}^{-1}$ in this case involves approximating $\mathcal{I}_{\eta, \theta}$ and adjusting $\mathcal{I}_{\eta, \eta}$, as described in \citet[Appendix B.3]{hougaard2012analysis} and \cite{putter2015dynamic}.

Confidence intervals for the conditional cumulative hazard are obtained from the part of the variance-covariance matrix corresponding to $\lambda_0(\cdot)$, and confidence intervals for $\Lambda_0(t) = \sum_{s\leq t} \lambda_0(t)$ are obtained with the usual formula. For confidence intervals, the delta method is used to calculate a symmetric confidence interval for $\log \Lambda_0(t)$ for all $t$, which is then exponentiated.

%\bibliographystyle{jss}

\bibliography{mybib}{}

% This can also be expressed in a combinatorial form. Consider $\Pi = P\left(\left\{1, ..., n \right\}\right)$ the set of all partitions of a set with $n$ elements. For any partition, we define a function $h(\pi)$ that determines the lengths of the elements of the partition $\pi$.
% If the elements $B \in \pi$ are sets of sizes $(b_1, ...,  b_B)$ then $h$ gives a vector of lengths that is ordered increasingly. For example, For the set $\left\{1,2,3 \right\}$, for the partition $h(\left\{1,2,3\right\}) = (0,0,3)$ and $h(\left\{1,2\right\}, \left\{ 3\right\}) = (0,1,2)$.

% To formalize this, we define
% $$
% S_n = \left\{\mathbf{m} \in \mathbb{N}^k   \vert m_{i-1} \leq m_i \,\,\forall i \geq 2 \,\,\,  \text{and} \,\,\sum_{i = 1}^k m_i = n\right\},
% $$
%  the set of increasingly ordered integer vectors of length $n$ that sum up to $n$ (with $S_1 = \left\{(1)\right\}$). This is in fact the image of $h$.
% Now, for each $s \in S_n$ the number of elements in the set $\left\{\pi \in \Pi | h(\pi) = s \right\}$ can be counted

%  Then we have $h:\Pi \rightarrow S_n$ with
% $h(\pi) = (min(|))$
% $$ m_{\pi \in \Pi} \prod_{B \in \pi} g^{(\vert B \vert)} (c)
% $$
% where $\Pi$ is the set of all partitions of the set , and $B \in \pi$ is a ``bloc'' of a partition.



% For every $k \geq 2$,
% $$
% S_k = \left\{\mathbf{x} \in \mathbb{N}^k   \vert x_{i-1} \leq x_i \,\,\forall i \geq 2 \,\,\,  \text{and} \,\,\sum_{i = 1}^k x_i = k\right\},
% $$
% where with $x_i$ we denote the $i$-th element of $\mathbf{x}$. For $k = 1$, we have $S_1 = \left\{(1)\right\}$.
% We have $S_2 = \left\{ (1,1), \,(0,2) \right\}$ and $S_3 = \left\{ (0,0,3), (0,1,2), (1,1,1) \right\}$ and so on.
% In other words, $S_k$ describes all sets of integers that sum up to $k$. We adopt the convention that $g^{(0)}(c) = 1$ and that $g^{(k)}(c) = \frac{d^k}{dc^k} g(c)$.
% Then the $k$-th derivative of the Laplace transform can be expressed as
% $$
% \mathcal{L}^{(k)}(c) = \sum_{\mathbf{x} \in S_k}  h(\mathbf{x}) \prod_{i = 1}^k g^{(x_i)}(c)
% $$
% where $h(\mathbf{x})$ is a function that takes values in $\mathbb{N} / \left\{ 0  \right\}$.
\end{document}
