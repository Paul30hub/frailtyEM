---
title: "Shared frailty models"
author: "Theodor Adrian Balan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## The likelihood
The type of data that these models can be applied on is usually consisting of either clustered failures or recurrent events. In both cases we will refer to a unit which has the same value of the frailty as a cluster. The process intensity can be written as

$$
\lambda_i(t) = z_i\exp(\gamma'\mathbf{x}_i(t))\lambda_0(t)
$$
Because the Andersen-Gill format is required for the input of the data, also left truncated data and time-dependent covariates are supported.

The likelihood for cluster $i$ can be written as
$$
\ell_i(\theta, \gamma, \lambda_0) = \prod_j \left(\exp(\gamma'\mathbf{x}_i(t_{ij}))\lambda_0(t_{ij})\right)^{\delta_{ij}} \mathrm{E} \left[z_i^{n_i} \exp(-z_i \Lambda_i)\right]
$$
where $\delta_{ij}$ is the event indicator, $n_i$ is the number of events in cluster $i$ and $\Lambda_i$ is the total hazard accumulated in cluster $i$. For example, for recurrent events, that is 
$$
\Lambda_i = \int_0^\infty Y_i(t) \exp(\gamma'\mathbf{x}_i(t))\lambda_0(t) dt
$$
and for clustered failures it is 
$$
\Lambda_i = \sum_k\int_0^\infty Y_{ik}(t) \exp(\gamma'\mathbf{x}_{ik}(t))\lambda_0(t) dt
$$
where the summing is over the individuals in cluster $i$. 

## Estimation

We define the profile likelihood as 
$$
\ell(\theta) = \mathrm{argmax}_{\gamma, \lambda_0} \ell(\theta, \gamma, \lambda_0)
$$
The maximization is a two-step procedure. The "inner" problem is to maximize $\ell(\theta, \gamma, \lambda_0)$ for a fixed $\theta$ with respect to $\gamma, \lambda_0$. The "outer" problem is to maximize $\ell(\theta)$. 

### The inner problem
The EM algorithm used to maximize $\ell(\theta, \gamma, \lambda_0)$ for a fixed $\theta$ takes place in `em_fit.R`. This function does not check any inputs and is not meant to be used outside its wrapper, `emfrail()`. 
Essentially, the function codes the distribution and the parameters in the variable `.pars` and then jumps straight to the EM algorithm. The convergence of this EM algorithm is set by the `.control` argument: namely, the `eps` determines when to actually detect convergence, and the `maxit` to stop the iterations after a given number of iterations. 

The E step is performed by calling the function `Estep` which resides in the C++ part of the package. 
The Laplace transform of $Z_i$ can be written as
$$
\mathcal{L}(C_i) = \mathrm{E}\left[\exp(-z C_i)\right] = \exp(-\alpha \psi (C_i; \beta))
$$
For the E step, what has to be calculated is
$$
z_i = \frac{E[z ^ N_{i+1} \exp(-z \Lambda_i)]}{E[z ^ N_{i} \exp(-z \Lambda_i)]} = 
\frac{(-1)^{N_{i} + 1} \mathcal{L}^{(N_i + 1)}(c) }{(-1)^{N_{i}} \mathcal{L}^{(N_i )}(c) }
$$
and note that the denominator in this case determines the contribution to the likelihood from cluster $i$.

The derivatives of the Laplace transform can be written according to the derivatives of $\psi$ like this:
$$
\mathcal{L}'(c; \alpha, \beta) = - \alpha \psi'(c; \beta) \mathcal{L}(c; \alpha, \beta)
$$
$$
\mathcal{L}''(c; \alpha, \beta) = \left[(- \alpha \psi'(c; \beta)) (- \alpha \psi'(c; \beta)) + (- \alpha \psi''(c; \beta))\right] \mathcal{L}(c; \alpha, \beta)
$$

To make the connection with the quantity that we are interested in, when we take odd derivatives, we need to multiply everything with a -1. That happens 
in the main `Estep()` function. 
The -1 that comes from every $-\alpha$ is done after the calculation of every element of the sum, in `findsums()`. 
The other bunch of $-1$ that may come from the derivatives of $\psi$, those are added in the `exponent_()` functions. 



### M step
The M step uses the maximizer for Cox models available in the `survival` package, `agreg.fit`. 
In the default `agreg.fit`, for the Andersen - Gill regression, if there is no offset specified, this is the same as taking `offset = 0`. 

NOTE: When you ask for the linear predictors, `R` actually returns this:
`lp  <- as.vector(x %*% coef + offset - sum(coef * colMeans(x)))`
In other words, the covariate vector is centered but not the offset.

To extract the hazard estimates, an internal function `getchz` is used, which resides in `emfrail_aux.R`. 

### Left truncation


## Outer problem



## Standard errors


## On the PVF family and parametrizations

### gamma
For the gamma distribution, we have $\psi(c) = \log(\beta + c) - \log(\beta)$ and derivatives are:
$$
\psi'(c) = (\beta + c)^{-1}
$$
$$
\psi''(c) = (-1)(\beta +c)^{-2}
$$

and so on, until
$$
\psi^{(k)} (c) = (-1)^{k-1} (k-1)! (\beta + c)^{-k}
$$

To parametrize it with one parameter $\theta > 0$ (which is 1/variance) we have the restriction $\beta = \alpha = \theta$, i.e. mean equals 1.

#### Fast fit
For fast fitting, we see that
$$
\mathcal{L}(c) = \beta^\alpha (\beta + c)^{-\alpha}
$$
and then we can write all the derivatives like:
$$
\mathcal{L}^{(k)}(c) = \beta^\alpha (\beta + c)^{-\alpha - k} \Gamma(\alpha + k) / \Gamma(\alpha).
$$


### stable
For the positive stable distribution, we have $\beta \in (0,1)$ (if $\beta = 1$ then it means that the marginal hazards are equal to the conditional hazards; if we have conditional covariate effect $\gamma$ then the marginal effect is $\beta \gamma$ so it is attenuated; predictive hazard ratios decrease to 1 over time)
$$
\psi(c) = c^\beta
$$
and
$$
\psi'(c) = \beta c^{\beta - 1}
$$
and
$$
\psi''(c) = \beta (\beta - 1) c^{\beta - 1}
$$
Now the problem is that $\beta - 1 < 0$ and then we run into trouble with the gamma function that is not defined for negative values. Because of that, we write it like this:
$$
\psi^{(k)}(c) = \frac{\Gamma(k - \beta)}{\Gamma(1 - \beta)} (-1)^{k - 1} c^{\beta - 1}
$$
To parametrize it with one parameter $\theta > 1$ (note: has to be larger than 1!) we have $\beta = 1 - 1/\theta$ and $\alpha = \theta / ( \theta - 1)$. The idea is that we let the user give a $\theta>0$, and the program increments this by 1 anyway. 

#### Fine fit
Fine Glidden and Lee proposed in a paper in JRSS B (2003) to use the fact that $\beta$ would be should be a weighed average between two hazard ratios: a *very* conditional one, obtained from stratifying for each cluster and a marginal one, obtained from a normal Cox model. However, this seems to work well in only a few scenarios.

They also assume that covariates vary within clusters and that at the marginal and conditional level we have the same covariates.

#### Martinussen fit
Martinussen and Pipper proposed in a paper in LiDA in 2005. A property of the stable model is that the marginal baseline hazard is
$$
h_0(t) = \alpha \lambda_0(t) \Lambda_0(t)^{\alpha - 1}.
$$
then we have that the baseline cumulative hazard is
$$
H_0(t) = \Lambda_0(t)^\alpha
$$

Then we can write the log-likelihood (conditional) with 
$$
\lambda_0(t) = d (H_0(t))^{1/\alpha}
$$

Then they say like that: the marginal hazard is the conditional times $E(z_i|t)$ with the estimate of $z_i$ at time $t$. They construct a likelihood with this $z_i|t$ calculated with an estimator of $H_0(t)$ under independence. Then they profile out $\lambda_0(t)$ like in a Cox model.

### pvf
For the pvf distribution with $m > -1$ and $m!=0$, we have the Laplace transform
$$
\mathcal{L}(c) = \exp \left[- \alpha \mathrm{sign}(m) (1 - \beta^m (\beta + c)^{-m}) \right]
$$
with $\alpha > 0$, $\beta > 0$. For positive $m$, that is:
$$
\psi(c) = (1 - \beta^m (\beta + c)^{-m}) 
$$
and
$$
\psi'(c) = -\beta^m (\beta + c)^{-m-1} (-m)
$$
The mean of the distribution is minus the 1st derivative of the Laplace transform in 0, which is
$$
\alpha \psi'(0) \mathcal{L}(0) = \frac{\alpha}{\beta} m.
$$
Also
$$
\psi''(c) = -\beta^m (\beta + c)^{-m-2} (-m-1) (-m)
$$
And we have the second moment of the distribution the second derivative in 0, 
$$
EZ^2 = \alpha^2 \psi'^2(0) -\alpha \psi''(0) = \frac{\alpha^2}{\beta^2}m^2 + \frac{\alpha}{\beta^2} m(m+1)= 1^2 + \frac{\alpha}{\beta^2}m(m+1).
$$
We have
$$
\psi^{(k)}(c) = -\beta^m (\beta + c)^{-m-k} (-1)^{k+1} \frac{\Gamma(m + k)}{\Gamma(m)}.
$$

Now for the parametrization we set the mean equal to 1, which is done by having a $\theta > 0$ and
$\beta = (m + 1) \theta$ and
$\alpha = \frac{m + 1}{m} \theta.$ With this, we have that the mean is 1 and the variance
$$
VarZ = 1/\theta.
$$
So how about when $m<0$? Well this implies that also $\alpha < 0$, and since the $\mathrm{sign}(m) = -1$, we might as well take $\alpha = \vert \frac{m + 1}{m} \theta \vert$. Then the only correction that is still needed is a $-1$ in the $\mathcal{L}$.

#### Inverse Gaussian
A particular case of the PVF distribution is the Inverse Gaussian, obtained when $ m = -1/2$, resulting in $\beta = \theta / 2$ and $\alpha = \theta$. The resulting Laplace transform is
$$
\mathcal{L}(c) = \exp \left\{\theta \left(1 - \sqrt{1 + 2c/\theta }\right) \right\}
$$
The $k$-th derivative of this can be written as
$$
\mathcal{L}^{(k)}(c) = (-1)^k \left(\frac{2}{\theta} c + 1\right)^{-k/2} \frac{\mathcal{K}_{k - 1/2} \left(\sqrt{ 2\theta \left(c + \frac{\theta}{2}\right)}\right)}{\mathcal{K}_{1/2} \left( \sqrt{ 2\theta \left(c + \frac{\theta}{2}\right)}\right)}
$$
where $\mathcal{K}$ is the modified Bessel function of the second kind.

## Ascertainment
There is a situation when the data set migt come from some ascertainment scheme that prevents individuals from being observed from time 0. For this example, I supress the subscript $i$ for the cluster. 

Now assume that observation $j$ in the cluster is in the data set if $A_j$ happens, and we call 
$$
A = A_1 \cup A_2 ... \cup A_n
$$
Then by Bayes' theorem the distribution of the frailty is given by 
$$
f_{Z|A}(z) = \frac{P(A|z) f(z)}{\int_z P(A|z) f(z) dz} = \frac{P(A|z)f(z)}{E_Z[P(A|z)]}
$$
Now this has Laplace transform
$$
\mathcal{L}_{Z|A}(c) = \frac{E_Z[e^{-cz}P(A|z)]}{E_Z[P(A|z)]}
$$
So basically it all boils down to writing $P(A|z)$. 

### Left truncation
In the left truncation scenario, i.e. where 
$$
P(A_j|z) = P(T_j >t_j|z) = E[\exp(-z \Lambda_{Lj})]
$$
and we have
$P(A|z) = \prod_j P(A_j|z)$ with $\Lambda_{Lj}$ the risk accumulated until the entry time $t_{Lj}$. Then denote 
$$
\Lambda_{L} = \sum_j \Lambda_{Lj}
$$
which can be seen as a sort of missing risk. 

In that case we see that
$$
\mathcal{L}_{Z|A}(c) = \frac{E_Z[e^{-z (c + \Lambda_{L})}]}{E_Z[e^{-z\Lambda_{L}}]}
$$
which means that
$$
\mathcal{L}_{Z|A}(c) = \frac{\mathcal{L}_Z(c + \Lambda_{L})}{\mathcal{L}_Z(\Lambda_L)}
$$


In the PVF framework, the E step must be replaced with a modified Laplace transform
$$
\mathcal{L}_{Z|A}(c) = \frac{\exp(-\alpha \psi(c + \Lambda_L))}{\exp(-\alpha \psi(\Lambda_L))},
$$
i.e.
$$
\mathcal{L}_{Z|A}(c) = \exp(-\alpha [\psi(c + \Lambda_L) - \psi(\Lambda_L)])
$$
which means that the conditional distribution is still part of the PVF family with
$$
\tilde{\psi} = \psi(c + \Lambda_L) - \psi(\Lambda_L).
$$

#### gamma frailty
For the gamma distribution recall that $\psi(x) = \log(\beta + c) - \log(\beta)$ so then
$$
\mathcal{L}_{Z|A}(c) = \exp\left[-\alpha \left(\log(\beta + \Lambda_L + c) - \log(\beta + \Lambda_L)\right)\right]
$$
which corresponds to a change in parameters with $\beta \rightarrow \beta + \Lambda_L$. 

We have that 
$$
\mathcal{L}_{Z|A}(c) = (\beta + \Lambda_L)^\alpha (\beta + \Lambda_L + c)^{-\alpha}
$$
so the $k$-th derivative is
$$
\mathcal{L}_{Z|A}^{(k)}(c) = \frac{(\beta + \Lambda_L)^\alpha}{(\beta + \Lambda_L + c)^{\alpha + k}} \frac{\Gamma(\alpha +k)}{\Gamma(\alpha)}.
$$

#### pvf and stable
For the PVF and positive stable distributions, under left truncation, the resulting Laplace transform under left truncation is in the PVF family. For the positive stable and the Inverse Gaussian, the resulting distribution does not remain the same, but changes to something else in the PVF family as well. 
