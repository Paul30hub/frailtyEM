---
title: "Shared frailty models"
author: "Theodor Adrian Balan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## The likelihood
The type of data that these models can be applied on is usually consisting of either clustered failures or recurrent events. In both cases we will refer to a unit which has the same value of the frailty as a cluster. The process intensity can be written as

$$
\lambda_i(t) = z_i\exp(\gamma'\mathbf{x}_i(t))\lambda_0(t)
$$
Because the Andersen-Gill format is required for the input of the data, also left truncated data and time-dependent covariates are supported.

The likelihood for cluster $i$ can be written as
$$
\ell_i(\theta, \gamma, \lambda_0) = \prod_j \left(\exp(\gamma'\mathbf{x}_i(t_{ij}))\lambda_0(t_{ij})\right)^{\delta_{ij}} \mathrm{E} \left[z_i^{n_i} \exp(-z_i \Lambda_i)\right]
$$
where $\delta_{ij}$ is the event indicator, $n_i$ is the number of events in cluster $i$ and $\Lambda_i$ is the total hazard accumulated in cluster $i$. For example, for recurrent events, that is 
$$
\Lambda_i = \int_0^\infty Y_i(t) \exp(\gamma'\mathbf{x}_i(t))\lambda_0(t) dt
$$
and for clustered failures it is 
$$
\Lambda_i = \sum_k\int_0^\infty Y_{ik}(t) \exp(\gamma'\mathbf{x}_{ik}(t))\lambda_0(t) dt
$$
where the summing is over the individuals in cluster $i$. 

## Estimation

We define the profile likelihood as 
$$
\ell(\theta) = \mathrm{argmax}_{\gamma, \lambda_0} \ell(\theta, \gamma, \lambda_0)
$$
The maximization is a two-step procedure. The "inner" problem is to maximize $\ell(\theta, \gamma, \lambda_0)$ for a fixed $\theta$ with respect to $\gamma, \lambda_0$. The "outer" problem is to maximize $\ell(\theta)$. 

### The inner problem
The EM algorithm used to maximize $\ell(\theta, \gamma, \lambda_0)$ for a fixed $\theta$ takes place in `em_fit.R`. This function does not check any inputs and is not meant to be used outside its wrapper, `emfrail()`. 
Essentially, the function codes the distribution and the parameters in the variable `.pars` and then jumps straight to the EM algorithm. The convergence of this EM algorithm is set by the `.control` argument: namely, the `eps` determines when to actually detect convergence, and the `maxit` to stop the iterations after a given number of iterations. 

The E step is performed by calling the function `Estep` which resides in the C++ part of the package. 
The Laplace transform of $Z_i$ can be written as
$$
\mathcal{L}(C_i) = \mathrm{E}\left[\exp(-z C_i)\right] = \exp(-\alpha \psi (C_i; \beta))
$$
For the E step, what has to be calculated is
$$
z_i = \frac{E[z ^ N_{i+1} \exp(-z \Lambda_i)]}{E[z ^ N_{i} \exp(-z \Lambda_i)]} = 
\frac{(-1)^{N_{i} + 1} \mathcal{L}^{(N_i + 1)}(c) }{(-1)^{N_{i}} \mathcal{L}^{(N_i )}(c) }
$$
and note that the denominator in this case determines the contribution to the likelihood from cluster $i$.

The derivatives of the Laplace transform can be written according to the derivatives of $\psi$ like this:
$$
\mathcal{L}'(c; \alpha, \beta) = - \alpha \psi'(c; \beta) \mathcal{L}(c; \alpha, \beta)
$$
$$
\mathcal{L}''(c; \alpha, \beta) = \left[(- \alpha \psi'(c; \beta)) (- \alpha \psi'(c; \beta)) + (- \alpha \psi''(c; \beta))\right] \mathcal{L}(c; \alpha, \beta)
$$

To make the connection with the quantity that we are interested in, when we take odd derivatives, we need to multiply everything with a -1. That happens 
in the main `Estep()` function. 
The -1 that comes from every $-\alpha$ is done after the calculation of every element of the sum, in `findsums()`. 
The other bunch of $-1$ that may come from the derivatives of $\psi$, those are added in the `exponent_()` functions. 

### M step
The M step uses the maximizer for Cox models available in the `survival` package, `agreg.fit`. 
In the default `agreg.fit`, for the Andersen - Gill regression, if there is no offset specified, this is the same as taking `offset = 0`. 

NOTE: When you ask for the linear predictors, `R` actually returns this:
`lp  <- as.vector(x %*% coef + offset - sum(coef * colMeans(x)))`
In other words, the covariate vector is centered but not the offset.

To extract the hazard estimates, an internal function `getchz` is used, which resides in `emfrail_aux.R`. 

## Outer problem



## Standard errors


## On the PVF family and parametrizations

### gamma
For the gamma distribution, we have $\psi(c) = \log(\beta + c) - \log(\beta)$ and derivatives are:
$$
\psi'(c) = (\beta + c)^{-1}
$$
$$
\psi''(c) = (-1)(\beta +c)^{-2}
$$

and so on, until
$$
\psi^{(k)} (c) = (-1)^{k-1} (k-1)! (\beta + c)^{-k}
$$

To parametrize it with one parameter $\theta > 0$ (which is 1/variance) we have the restriction $\beta = \alpha = \theta$, i.e. mean equals 1.

#### Fast fit
For fast fitting, we see that
$$
\mathcal{L}(c) = \beta^\alpha (\beta + c)^{-\alpha}
$$
and then we can write all the derivatives like:
$$
\mathcal{L}^{(k)}(c) = \beta^\alpha (\beta + c)^{-\alpha - k} \Gamma(\alpha + k) / \Gamma(\alpha)
$$
To obtain this with the frailty variance, we would take $\theta = 


### stable
For the positive stable distribution, we have $\beta \in (0,1)$ (if $\beta = 1$ then it means that the marginal hazards are equal to the conditional hazards; if we have conditional covariate effect $\gamma$ then the marginal effect is $\beta \gamma$ so it is attenuated; predictive hazard ratios decrease to 1 over time)
$$
\psi(c) = c^\beta
$$
and
$$
\psi'(c) = \beta c^{\beta - 1}
$$
and
$$
\psi''(c) = \beta (\beta - 1) c^{\beta - 1}
$$
Now the problem is that $\beta - 1 < 0$ and then we run into trouble with the gamma function that is not defined for negative values. Because of that, we write it like this:
$$
\psi^{(k)}(c) = \frac{\Gamma(k - \beta)}{\Gamma(1 - \beta)} (-1)^{k - 1} c^{\beta - 1}
$$
To parametrize it with one parameter $\theta > 1$ (note: has to be larger than 1!) we have $\beta = 1 - 1/\theta$ and $\alpha = \theta / ( \theta - 1)$. The idea is that we let the user give a $\theta>0$, and the program increments this by 1 anyway. 

#### Fine fit
Fine Glidden and Lee proposed in a paper in JRSS B (2003) to use the fact that $\beta$ would be should be a weighed average between two hazard ratios: a *very* conditional one, obtained from stratifying for each cluster and a marginal one, obtained from a normal Cox model. However, this seems to work well in only a few scenarios.

They also assume that covariates vary within clusters and that at the marginal and conditional level we have the same covariates.

#### Martinussen fit
Martinussen and Pipper proposed in a paper in LiDA in 2005. A property of the stable model is that the marginal baseline hazard is
$$
h_0(t) = \alpha \lambda_0(t) \Lambda_0(t)^{\alpha - 1}.
$$
then we have that the baseline cumulative hazard is
$$
H_0(t) = \Lambda_0(t)^\alpha
$$

Then we can write the log-likelihood (conditional) with 
$$
\lambda_0(t) = d (H_0(t))^{1/\alpha}
$$

Then they say like that: the marginal hazard is the conditional times $E(z_i|t)$ with the estimate of $z_i$ at time $t$. They construct a likelihood with this $z_i|t$ calculated with an estimator of $H_0(t)$ under independence. Then they profile out $\lambda_0(t)$ like in a Cox model.

### pvf
For the pvf distribution with $m > -1$ and $m!=0$, we have the Laplace transform
$$
\mathcal{L}(c) = \exp \left[- \alpha \mathrm{sign}(m) (1 - \beta^m (\beta + c)^{-m}) \right]
$$
with $\alpha > 0$, $\beta > 0$. For positive $m$, that is:
$$
\psi(c) = (1 - \beta^m (\beta + c)^{-m}) 
$$
and
$$
\psi'(c) = -\beta^m (\beta + c)^{-m-1} (-m)
$$
The mean of the distribution is minus the 1st derivative of the Laplace transform in 0, which is
$$
\alpha \psi'(0) \mathcal{L}(0) = \frac{\alpha}{\beta} m.
$$
Also
$$
\psi''(c) = -\beta^m (\beta + c)^{-m-2} (-m-1) (-m)
$$
And we have the second moment of the distribution the second derivative in 0, 
$$
EZ^2 = \alpha^2 \psi'^2(0) -\alpha \psi''(0) = \frac{\alpha^2}{\beta^2}m^2 + \frac{\alpha}{\beta^2} m(m+1)= 1^2 + \frac{\alpha}{\beta^2}m(m+1).
$$
We have
$$
\psi^{(k)}(c) = -\beta^m (\beta + c)^{-m-k} (-1)^{k+1} \frac{\Gamma(m + k)}{\Gamma(m)}.
$$

Now for the parametrization we set the mean equal to 1, which is done by having a $\theta > 0$ and
$\beta = (m + 1) \theta$ and
$\alpha = \frac{m + 1}{m} \theta.$ With this, we have that the mean is 1 and the variance
$$
VarZ = 1/\theta.
$$
So how about when $m<0$? Well this implies that also $\alpha < 0$, and since the $\mathrm{sign}(m) = -1$, we might as well take $\alpha = \vert \frac{m + 1}{m} \theta \vert$. Then the only correction that is still needed is a $-1$ in the $\mathcal{L}$.


## Ascertainment
There is a situation when the data set migt come from some ascertainment scheme that prevents individuals from being observed from time 0. For this example, I supress the subscript $i$ for the cluster. 

Now assume that observation $j$ in the cluster is in the data set if $A_j$ happens, and we call 
$$
A = A_1 \cup A_2 ... \cup A_n
$$
Then by Bayes' theorem the distribution of the frailty is given by 
$$
f_{Z|A}(z) = \frac{P(A|z) f(z)}{\int_z P(A|z) f(z) dz} = \frac{P(A|z)f(z)}{E_Z[P(A|z)]}
$$
Now this has Laplace transform
$$
\mathcal{L}_{Z|A}(c) = \frac{E_Z[e^{-cz}P(A|z)]}{E_Z[P(A|z)]}
$$
So basically it all boils down to writing $P(A|z)$. 

### Left truncation
In the left truncation scenario, i.e. where 
$$
P(A_j|z) = P(T_j >t_j|z) = E[\exp(-z \Lambda_{Lj})]
$$
and we have
$P(A|z) = \prod_j P(A_j|z)$ with $\Lambda_{Lj}$ the risk accumulated until the entry time $t_{Lj}$. Then denote 
$$
\Lambda_{L} = \sum_j \Lambda_{Lj}
$$
which can be seen as a sort of missing risk. 

In that case we see that
$$
\mathcal{L}_{Z|A}(c) = \frac{E_Z[e^{-z (c + \Lambda_{L})}]}{E_Z[e^{-z\Lambda_{L}}]}
$$
which means that
$$
\mathcal{L}_{Z|A}(c) = \frac{\mathcal{L}_Z(c + \Lambda_{L})}{\mathcal{L}_Z(\Lambda_L)}
$$
Now if we are in the PVF framework,


$$
\mathcal{L}_{Z|A}(c) = \frac{\exp(-\alpha \psi(c + \Lambda_L))}{\exp(-\alpha \psi(\Lambda_l))}

$$
so 

$$
\mathcal{L}_{Z|A}(c) = \exp(-\alpha [\psi(c + \Lambda_L) - \psi(\Lambda_L)])
$$

#### gamma frailty
For the gamma distribution recall that $\psi(x) = \log(\beta + c) - \log(\beta)$ so then
$$
\mathcal{L}_{Z|A}(c) = \exp\left[-\alpha \log(\beta + \Lambda_L + c) - \log(\beta + \Lambda_L)\right] 
))
$$
So for the gamma distribution the essence is a change in parameters $\beta \rightarrow \beta + \Lambda_L$. 
In other words, the adjustment that has to happen is that the E step, if there are $k$ events,
$$
z = \frac{\alpha + k}{\beta + \Lambda_L + \Lambda}
$$
which is funny because this is as if taking a person coming from the time 0.

We have that 
$$
\mathcal{L}_{Z|A}(c) = (\beta + \Lambda_L)^\alpha (\beta + \Lambda_L + c)^{-\alpha}
$$
so the $k$-th derivative is
$$
\mathcal{L}_{Z|A}^{(k)}(c) = \frac{(\beta + \Lambda_L)^\alpha}{(\beta + \Lambda_L + c)^{\alpha + k}} \frac{\Gamma(\alpha +k)}{\Gamma(\alpha)} 
$$
so this is what we should have as likelihood contribution.

Vignettes are long form documentation commonly included in packages. Because they are part of the distribution of the package, they need to be as compact as possible. The `html_vignette` output type provides a custom style sheet (and tweaks some options) to ensure that the resulting html is as small as possible. The `html_vignette` format:

- Never uses retina figures
- Has a smaller default figure size
- Uses a custom CSS stylesheet instead of the default Twitter Bootstrap style

## Vignette Info

Note the various macros within the `vignette` section of the metadata block above. These are required in order to instruct R how to build the vignette. Note that you should change the `title` field and the `\VignetteIndexEntry` to match the title of your vignette.

## Styles

The `html_vignette` template includes a basic CSS theme. To override this theme you can specify your own CSS in the document metadata as follows:

    output: 
      rmarkdown::html_vignette:
        css: mystyles.css

## Figures

The figure sizes have been customised so that you can easily put two images side-by-side. 

```{r, fig.show='hold'}
plot(1:10)
plot(10:1)
```

You can enable figure captions by `fig_caption: yes` in YAML:

    output:
      rmarkdown::html_vignette:
        fig_caption: yes

Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**.

## More Examples

You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`.

```{r, echo=FALSE, results='asis'}
knitr::kable(head(mtcars, 10))
```

Also a quote using `>`:

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
