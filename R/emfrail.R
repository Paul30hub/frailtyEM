#' Fitting shared frailty models wiht the EM algorithm
#'
#' @importFrom survival Surv
#' @importFrom stats approx coef model.frame model.matrix pchisq printCoefmat
#' @importFrom magrittr "%>%"
#' @useDynLib frailtoys
#' @include em_fit.R
#' @include dist_to_pars.R
#'
#' @param .data A data frame in which the formula argument can be evaluated
#' @param .formula A formula that contains on the left hand side an object of the type \code{Surv} in the Andersen-Gill format, and on the right hand side a \code{+cluster(id)} statement
#' @param .distribution An object as created by \code{\link{emfrail_distribution}} which defines the frailty distribution. See Details.
#' @param .control An object as created by \code{\link{emfrail_control}}
#'
#' @return An object of the class \code{emfrail}, that is in fact a list which contains (1) the object returned by the
#' "outer maximization" from \code{optimx}, (2) the object with all the estimates returned by the "inner maximization",
#' (3) the log-likelihood of the Cox model without frailty, (4) the variance-covariance matrix adjusted for the uncertainty in the
#' outer maximization, and (5,6,7) are copies of the original input arguments: .formula, .distribution and .control.
#' @export
#'
#' @details The \code{emfrail} function fits shared frailty models for processes which have intensity
#' \deqn{\lambda(t) = z \lambda_0(t) \exp(\gamma' \mathbf{x})}
#' with a non-parametric (Breslow) baseline intensity \eqn{\lambda_0(t)}.
#' The distribution of \eqn{z} is described by one parameter \eqn{\theta} (see the vignette on the available parametrizations).
#'
#' The "inner" problem is that, for a fixed \eqn{\theta}, the log-likelihood \eqn{l_\theta} must be maximized with respect to
#' \eqn{\lambda_0(t)} and \eqn{\gamma}. This is done via an EM algorithm, which relies in the M step on the \code{agreg} function from the \code{survival} package.
#' From this EM algorithm, standard errors are obtained using Louis' formula.
#' The "outer" problem is that of maximizing \eqn{l_\theta} with respect to \eqn{\theta}. That is done using the \code{optimx} function
#' from the \code{optimx} package, which also provides the corresponding Hessian matrix.
#' The outer loop can be bypassed with the correct specification in the
#' \code{.control} argument, and the likelihood for the \eqn{\theta} as specified in the \code{.distribution} argument
#' is then returned.
#' The frailty distribution is determined by the \code{.distribution} argument. This must be generated by a call from
#' \code{emfrail_distribution()}.
#'
#' The family of supported distributions can be one of gamma, positive stable or PVF (power-variance-family).
#' @note Some possible problems may appear when the maximum likelihood estimate lies on the border of the parameter space. Usually, this will happen
#' when the "outer" parameter MLE is infinity (i.e. variance 0 in case of gamma and PVF). For small enough values of \eqn{1/\theta} the log-likelihood
#' of the Cox model will be returned. The tolerance of this option can be tweaked in \code{emfrail_control()}.
#'
#' @examples
#' dat <- survival::rats
#' m1 <- emfrail(.data =  dat, .formula = Surv(rep(0, nrow(dat)), time, status) ~  rx + sex + cluster(litter),
#' .distribution = emfrail_distribution(dist = "gamma"))
#' m1
#'
#' m2 <- emfrail(.data =  dat, .formula = Surv(rep(0, nrow(dat)), time, status) ~  rx + sex + cluster(litter),
#'               .distribution = emfrail_distribution(dist = "pvf"))
#' m2
#'
#' m3 <- emfrail(.data =  dat, .formula = Surv(rep(0, nrow(dat)), time, status) ~  rx + sex + cluster(litter),
#'               .distribution = emfrail_distribution(dist = "stable"),
#'               .control = emfrail_control(verbose = FALSE))
#' m3
#'
#'
#'
#'
#' # draw the profile likelihood for different values of the frailty variance (gamma frailty)
#' fr_var <- c(0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)
#' prof_lik <- sapply(1/fr_var, function(th)
#'   -emfrail(.data =  dat, .formula = Surv(rep(0, nrow(dat)), time, status) ~  rx + sex + cluster(litter),
#'            .distribution = emfrail_distribution(frailtypar = th), .control = emfrail_control(opt_fit = FALSE))
#' )
#'
#' plot(fr_var, prof_lik, xlab = "frailty variance", ylab = "log-likelihood")
#'
#' # check with coxph:
#' prof_lik_cph <- sapply(fr_var, function(th)
#'   survival::coxph(data =  dat, formula = Surv(rep(0, nrow(dat)), time, status) ~ rx + sex + frailty(litter, theta = th),
#'                  method = "breslow")$history[[1]][[3]])
#'
#' lines(fr_var, prof_lik_cph, col = 2)
#'
#'
#' # simulated data ----------------------------------------------------------
#' set.seed(1)
#' x <- sample(c(0,1), 300, TRUE)
#' z <- rep(rgamma(100, 1, 1), each = 3)
#'
#' time <- rexp(300, rate = z * exp(0.5*x) )
#'
#' censtime <- 5
#' status <- rep(1, 300)
#'
#' status[time >= censtime] <- 0
#' time[status == 0] <- censtime
#' time0 <- rep(0, 300)
#'
#' dd <- data.frame(id = rep(1:100, each = 3), x = x, time0 = time0, time = time, status = status)
#'
#'
#' emfrail(.data =  dd, .formula = Surv(rep(0, nrow(dat)), time, status) ~  x + cluster(id),
#'         .distribution = emfrail_distribution( dist = "stable"),
#'         .control = emfrail_control(opt_fit = TRUE))
#'
#'
#' #Recurrent events --------------------------------------------------------
#'
#' data("bladder")
#' coxph(Surv(start, stop, status) ~ treatment + frailty(id), data = bladder1, method = "breslow")
#' mod2 <- emfrail(bladder1, Surv(start, stop, status) ~ treatment + cluster(id))
#'
#' # Plotting the baseline cumulative hazard / intensity
#'
#' with(mod2$res$haz,
#'      plot(time, cumhaz, type = "s", ylim = c(0, max(cumhaz) + 2 * max(se_chz_adj)),
#'      main = "Cumulative baseline hazard",
#'      ylab = "H0"))
#' with(mod2$res$haz, lines(time, cumhaz - 1.96*se_chz, col = 2))
#' with(mod2$res$haz, lines(time, cumhaz + 1.96*se_chz, col = 2))
#' with(mod2$res$haz, lines(time, cumhaz - 1.96*se_chz_adj, col = 3, lty = 2))
#' with(mod2$res$haz, lines(time, cumhaz + 1.96*se_chz_adj, col = 3, lty = 2))
#' legend(x = 0, y = 5, legend = c("95% CI", "adjusted 95% CI"), col = c(2,3), lty = c(1,2))
#'
#' # with ggplot2
#' library(ggplot2)
#' ggplot(mod2$res$haz, aes(x = time)) +
#' geom_ribbon(aes(ymin = cumhaz - 1.96*se_chz, ymax = cumhaz + 1.96*se_chz),  fill = "grey70") +
#'   geom_ribbon(aes(ymin = cumhaz - 1.96*se_chz_adj, ymax = cumhaz + 1.96*se_chz_adj),  fill = "pink", alpha = 0.2) +
#'   geom_step(aes(y = cumhaz)) +
#'   ggtitle("Cumulative baseline hazard") +
#'   ylab("H0")
#'
#'
#'  # Left truncation
#'  # simulate 300 clusters of size 5
#' set.seed(17)
#' x <- sample(c(0,1), 5 * 300, TRUE)
#' u <- rep(rgamma(300,1,1), each = 5)
#' stime <- rexp(5*300, rate = u * exp(x))
#' ltime <- runif(5 * 300)
#'
#' library(tidyverse)
#' d <- data.frame(id = rep(1:300, each = 5),
#'                 x = x,
#'                 stime = stime,
#'                 u = u,
#'                 ltime = ltime,
#'                 status = 1)
#'
#'
#' d1 <- d %>% filter(stime > ltime)
#'  # this is the same as the cph (naive):
#' mymod <- d1 %>%
#'   emfrail(Surv(ltime, stime, status)~ x + cluster(id), .control = emfrail_control(verbose = TRUE))
#'
#'  # this is the correct way here:
#' mymod <- d1 %>%
#'   emfrail(Surv(ltime, stime, status)~ x + cluster(id), .control = emfrail_control(verbose = TRUE),
#'           .distribution = emfrail_distribution(left_truncation = TRUE))
#'@seealso \code{\link{summary.emfrail}, \link{predict.emfrail}, \link{emfrail_distribution}, \link{emfrail_control}}


emfrail <- function(.data,
                    .formula,
                    .distribution = emfrail_distribution(),
                    .control = emfrail_control()) {

  if(!inherits(.distribution, "emfrail_distribution"))
    stop(".distribution argument misspecified; see ?emfrail_distribution()")

  if(!inherits(.control, "emfrail_control"))
    stop(".control argument misspecified; see ?emfrail_control()")

  if(isTRUE(.control$fast_fit)) {
    if(!(.distribution$dist %in% c("gamma", "pvf"))) {
      #message("fast_fit option only available for gamma and pvf with m=-1/2 distributions")
      .control$fast_fit <- FALSE
    }
    if(.distribution$dist == "pvf" & (.distribution$pvfm != -1/2)) {
      #message("fast_fit option only available for gamma and pvf with m=-1/2 distributions")
      .control$fast_fit <- FALSE
    }

    if(.distribution$dist == "pvf" & (.distribution$pvfm == -1/2) & .distribution$left_truncation ) {
      #message("fast_fit option not available with left truncation for the Inverse Gaussian")
      .control$fast_fit <- FALSE
    }
  }


  Call <- match.call()


  if(missing(.formula)  | missing(.data)) stop("Missing arguments")
  # if(missing(.distribution)) {
  #   .distribution <-  emfrail_distribution()
  #   print("default distribution")
  # }
  # if(missing(.control)) .control <- emfrail_control()

  cluster <- function(x) x
  mf <- model.frame(.formula, .data)

  # Identify the cluster and the ID column
  pos_cluster <- grep("cluster", names(mf))
  if(length(pos_cluster) != 1) stop("misspecified or non-specified cluster")
  id <- mf[[pos_cluster]]

  # Identify Surv object
  Y <- mf[[1]]
  if(!inherits(Y, "Surv")) stop("left hand side not a survival object")
  if(attr(Y, "type") != "counting") stop("use Surv(tstart, tstop, status)")





  # get the model matrix
  X1 <- model.matrix(.formula, .data)
  # this is necessary because when factors have more levels, pos_cluster doesn't correspond any more
  pos_cluster_X1 <- grep("cluster", colnames(X1))
  X <- X1[,-c(1, pos_cluster_X1), drop=FALSE]
  # note: X has no attributes, in coxph it does.


  # some stuff for creating the C vector, is used all along.
  # mcox also works with empty matrices, but also with NULL as x.
  mcox <- survival::agreg.fit(x = X, y = Y, strata = NULL, offset = NULL, init = NULL,
                              control = survival::coxph.control(),
                              weights = NULL, method = "breslow", rownames = NULL)

  # the "baseline" case // this will stay constant

  if(length(X) == 0) {
    newrisk <- 1
    exp_g_x <- matrix(rep(1, length(mcox$linear.predictors)), nrow = 1)
    g <- 0
    g_x <- t(matrix(rep(0, length(mcox$linear.predictors)), nrow = 1))

  } else {
    x2 <- matrix(rep(0, ncol(X)), nrow = 1, dimnames = list(123, dimnames(X)[[2]]))
    x2 <- (scale(x2, center = mcox$means, scale = FALSE))
    newrisk <- exp(c(x2 %*% mcox$coefficients) + 0)
    exp_g_x <- exp(mcox$coefficients %*% t(X))
    g <- mcox$coefficients
    g_x <- t(mcox$coefficients %*% t(X))

  }

  explp <- exp(mcox$linear.predictors) # these are with centered covariates

  # hh <- getchz(Y = Y, newrisk = newrisk, explp = explp)




  # cumhaz_line <- sapply(X = apply(as.matrix(Y[,c(1,2)]), 1, as.list),
  #                       FUN = function(x)  sum(hh$haz_tev[x$start <= hh$tev & hh$tev <= x$stop])) *
  #   exp_g_x # this is supposed to be without centered covariates.
  # #
  # #

  # #
  # basehaz_line <- hh$haz_tev[match(Y[,2], hh$tev)]

  # Cvec <- tapply(X = cumhaz_line,
  #                INDEX = id,
  #                FUN = sum)

  # the shorter way

  # new way of doing the things;

  nev_id <- rowsum(Y[,3], id) # nevent per id or am I going crazy

   # Cvec <- nev_id - as.vector(rowsum(mcox$residuals, id)) #WONG?

  #cumhaz_line_a <- (Y[,3] - mcox$residuals) # with covariates!



  # for the baseline hazard how the fuck is that gonna happen?
  # Idea: nrisk has the sum of elp who leave later at every tstop
  # esum has the sum of elp who enter at every tstart
  # indx groups which esum is right after each nrisk;
  # the difference between the two is the sum of elp really at risk at that time point.


  nrisk <- rev(cumsum(rev(rowsum(explp, Y[, ncol(Y) - 1]))))
  esum <- rev(cumsum(rev(rowsum(explp, Y[, 1]))))

  # the stuff that won't change
  death <- (Y[, ncol(Y)] == 1)
  nevent <- as.vector(rowsum(1 * death, Y[, ncol(Y) - 1])) # per time point
  time <- sort(unique(Y[,2])) # unique tstops

  # this gives the next entry time for each unique tstop (not only event)
  etime <- c(0, sort(unique(Y[, 1])),  max(Y[, 1]) + min(diff(time)))
  indx <- findInterval(time, etime, left.open = TRUE) # left.open  = TRUE is very important

  # this gives for every tstart (line variable) after which event time did it come
  # indx2 <- findInterval(Y[,1], time, left.open = FALSE, rightmost.closed = TRUE)
  indx2 <- findInterval(Y[,1], time)

  time_to_stop <- match(Y[,2], time)
  order_id <- findInterval(id, unique(id))

  atrisk <- list(death = death, nevent = nevent, nev_id = nev_id,
                 order_id = order_id, time = time, indx = indx, indx2 = indx2,
                 time_to_stop = time_to_stop)

  nrisk <- nrisk - c(esum, 0,0)[indx]

  # esum %>% length
  # indx %>% length
  #
  # esum
  # indx
  # # last indx is 548
  # esum

  haz <- nevent/nrisk * newrisk


  basehaz_line <- haz[atrisk$time_to_stop]

  cumhaz <- cumsum(haz)

  cumhaz_0_line <- cumhaz[atrisk$time_to_stop]
  cumhaz_tstart <- c(0, cumhaz)[atrisk$indx2 + 1]
  cumhaz_line <- (cumhaz_0_line - cumhaz_tstart)  * explp / newrisk

  Cvec <- rowsum(cumhaz_line, atrisk$order_id)


  # cumulative hazard?


  # getChz style

  # hh <- getchz(Y, newrisk, explp)
  # cumhaz_line_2 <- sapply(X = apply(as.matrix(Y[,c(1,2)]), 1, as.list),
  #                       FUN = function(x)  sum(hh$haz_tev[x$start <= hh$tev & hh$tev <= x$stop])) *
  #   exp_g_x
  #
  # which(as.numeric(cumhaz_line_2)[1] != cumhaz_line[1])
  # abline(0,1,col=2)

  #all.equal(hh$haz, haz)

  #Cvec_from0 - Cvec

  if(isTRUE(.distribution$left_truncation)) {
    #indx2 <- findInterval(Y[,1], time, left.open = TRUE)
    cumhaz_tstop <- cumsum(haz)
    cumhaz_tstart <- c(0, cumhaz_tstop)[indx2 + 1]

    Cvec_lt <- rowsum(cumhaz_tstart, order_id)
    # Cvec_lt <- tapply(X = cumhaz_tstart,
    #                   INDEX = id,
    #                   FUN = sum)
  } else Cvec_lt <- 0 * Cvec


  # a fit just for the log-likelihood;
  if(!isTRUE(.control$opt_fit)) {
    return(em_fit(logfrailtypar = log(.distribution$frailtypar),
           dist = .distribution$dist, pvfm = .distribution$pvfm,
           Y = Y, Xmat = X, atrisk = atrisk, basehaz_line = basehaz_line,
           mcox = list(coefficients = g, loglik = mcox$loglik),  # a "fake" cox model
           Cvec = Cvec, lt = .distribution$left_truncation,
           Cvec_lt = Cvec_lt,
          .control = .control))
  }


  # otherwise, the maximizer
  outer_m <- optimx::optimx(par = log(.distribution$frailtypar), fn = em_fit,
               hessian = TRUE,
               #lower = -10000, upper = 10000,
               method = .control$opt_control$method, #control = .control$opt_control$control,
               #control = list(trace = 10, save.failures = TRUE),
               dist = .distribution$dist, pvfm = .distribution$pvfm,
               Y = Y, Xmat = X, atrisk = atrisk, basehaz_line = basehaz_line,
               mcox = list(coefficients = g, loglik = mcox$loglik),  # a "fake" cox model
               Cvec = Cvec, lt = .distribution$left_truncation,
               Cvec_lt = Cvec_lt,
               .control = .control)

  message("Calculating final fit with information matrix...")

  inner_m <- em_fit(logfrailtypar = outer_m$p1,
                      dist = .distribution$dist, pvfm = .distribution$pvfm,
                      Y = Y, Xmat = X, atrisk = atrisk, basehaz_line = basehaz_line,
                      mcox = list(coefficients = g, loglik = mcox$loglik),  # a "fake" cox model
                      Cvec = Cvec, lt = .distribution$left_truncation,
                      Cvec_lt = Cvec_lt,
                      .control = .control, return_loglik = FALSE)

  # that the hessian
  h <- as.numeric(sqrt(1/(attr(outer_m, "details")[[3]]))/2)
  lfp_minus <- max(outer_m$p1 - h , outer_m$p1 - 5)
  lfp_plus <- min(outer_m$p1 + h , outer_m$p1 + 5)

  message("Calculating adjustment for information matrix...")


  final_fit_minus <- em_fit(logfrailtypar = lfp_minus,
                            dist = .distribution$dist, pvfm = .distribution$pvfm,
                            Y = Y, Xmat = X, atrisk = atrisk, basehaz_line = basehaz_line,
                            mcox = list(coefficients = g, loglik = mcox$loglik),  # a "fake" cox model
                            Cvec = Cvec, lt = .distribution$left_truncation,
                            Cvec_lt = Cvec_lt,
                            .control = .control, return_loglik = FALSE)

  final_fit_plus <- em_fit(logfrailtypar = lfp_plus,
                           dist = .distribution$dist, pvfm = .distribution$pvfm,
                           Y = Y, Xmat = X, atrisk = atrisk, basehaz_line = basehaz_line,
                           mcox = list(coefficients = g, loglik = mcox$loglik),  # a "fake" cox model
                           Cvec = Cvec, lt = .distribution$left_truncation,
                           Cvec_lt = Cvec_lt,
                           .control = .control, return_loglik = FALSE)

  # instructional: this should be more or less equal to the
  -(final_fit_plus$loglik + final_fit_minus$loglik - 2 * inner_m$loglik)/h^2

  # se_logtheta^2 / (2 * (final_fit$loglik -final_fit_plus$loglik ))

  deta_dtheta <- (c(final_fit_plus$coef, final_fit_plus$haz) -
    c(final_fit_minus$coef, final_fit_minus$haz)) / (2*h)

  #adj_se <- sqrt(diag(deta_dtheta %*% (1/(attr(opt_object, "details")[[3]])) %*% t(deta_dtheta)))

  vcov_adj = inner_m$Vcov + deta_dtheta %*% (1/(attr(outer_m, "details")[[3]])) %*% t(deta_dtheta)
  #

  # est_dist <- emfrail_distribution(dist = .distribution$dist,
  #                                  frailtypar = final_fit$frailtypar,
  #                                  pvfm = .distribution$pvfm
  # )


  res <- list(outer_m = outer_m,
              inner_m = inner_m,
              loglik_null = mcox$loglik[length(mcox$loglik)],
              # mcox = mcox,
              vcov_adj = vcov_adj,
              .formula,
              .distribution,
              .control
              )

# res <- list(outer_m = opt_object, # contains the maximization
#             inner_m = final_fit, # the inner object
#             mcox = mcox, # initial cox model
#             id = unique(id),
#             .distribution = .distribution, # the initial distribution call
#
#             #est_dis = est_dist,
#               res = list(loglik = final_fit$loglik, # obsolete ?
#                          dist = final_fit$dist, # in est_dist
#                          pvfm = final_fit$pvfm, # in est_dist
#                          theta = final_fit$frailtypar, # in est_dist
#
#                          # this should be here (the baseline at least).
#                          haz = data.frame(time = final_fit$haz$tev,
#                                           cumhaz = cumsum(final_fit$haz$haz_tev)),
#                                           #se = final_fit$se[(1 + length(final_fit$coef)):length(final_fit$se)]),
#
#                          # this should be here as well.
#                          z = data.frame(id = unique(id),
#                                         nev = atrisk$nev_id,
#                                         Lambda = final_fit$Cvec,
#                                         z = final_fit$estep[,1] / final_fit$estep[,2] ),
#                          # this SHOULD be here, really.
#                                         coef = final_fit$coef,
#
#                          # these things happen later anyway. Maybe that should be part of a summary object.
#                          se_coef = sqrt(diag(final_fit$Vcov)[seq_along(final_fit$coef)]),
#                          se_coef_adj = sqrt(diag(vcov_adj)[seq_along(final_fit$coef)] )),
#
#             # the initial Cox model is only good to have for the loglikelihood
#             mcox = mcox,
#             vcov = final_fit$Vcov,
#             vcov_adj = vcov_adj
#                )
  attr(res, "class") <- "emfrail"

  res


}
