#' Fitting shared frailty models wiht the EM algorithm
#'
#' @importFrom survival Surv
#' @importFrom stats approx coef model.frame model.matrix pchisq printCoefmat
#' @importFrom magrittr "%>%"
#' @useDynLib frailtoys
#' @include em_fit.R
#' @include emfrail_aux.R
#'
#' @param .data A data frame in which the formula argument can be evaluated
#' @param .formula A formula that contains on the left hand side an object of the type \code{Surv} in the Andersen-Gill format, and on the right hand side a \code{+cluster(id)} statement
#' @param .distribution (optional) An object as created by \code{\link{emfrail_distribution}} which defines the frailty distribution. See Details.
#' @param .control (optional) An object as created by \code{\link{emfrail_control}}
#'
#' @return An emfrail-type object, containing a lot of interesting information.
#' @export
#'
#' @details The \code{emfrail} function fits shared frailty models for processes which have intensity
#' \deqn{\lambda(t) = z \lambda_0(t) \exp(\gamma' \mathbf{x})}
#' with a non-parametric (Breslow) baseline intensity \eqn{\lambda_0(t)}.
#' The distribution of \eqn{z} is described by one parameter \eqn{\theta} (see the vignette on the available parametrizations).
#'
#' The "inner" problem is that, for a fixed \eqn{\theta}, the log-likelihood \eqn{l_\theta} must be maximized with respect to
#' \eqn{\lambda_0(t)} and \eqn{\gamma}. This is done via an EM algorithm, which relies in the M step on the \code{agreg} function from the \code{survival} package.
#' From this EM algorithm, standard errors are obtained using Louis' formula.
#' The "outer" problem is that of maximizing \eqn{l_\theta} with respect to \eqn{\theta}. That is done using the \code{optimx} function
#' from the \code{optimx} package, which also provides the corresponding Hessian matrix.
#' Several options exist, for example, to not do the outer loop and just maximize \eqn{l_\theta} for fixed \eqn{\theta}.
#'
#' The frailty distribution is determined by the \code{.distribution} argument. This must be generated by a call from
#' \code{emfrail_distribution()}.
#'
#' The family of supported distributions can be one of gamma, positive stable or PVF (power-variance-family).
#'
#' Some possible problems may appear when the maximum likelihood estimate lies on the border of the parameter space. Usually, this will happen
#' when the "outer" parameter MLE is infinity (i.e. variance 0 in case of gamma and PVF). For small enough values of \eqn{1/\theta} the log-likelihood
#' of the Cox model will be returned. The tolerance of this option can be tweaked in \code{emfrail_control()}.
#'
#' @examples
#' dat <- survival::rats
#' m1 <- emfrail(.data =  dat, .formula = Surv(rep(0, nrow(dat)), time, status) ~  rx + sex + cluster(litter),
#' .distribution = emfrail_distribution(dist = "gamma"))
#' m1
#'
#' m2 <- emfrail(.data =  dat, .formula = Surv(rep(0, nrow(dat)), time, status) ~  rx + sex + cluster(litter),
#'               .distribution = emfrail_distribution(dist = "pvf"))
#' m2
#'
#' m3 <- emfrail(.data =  dat, .formula = Surv(rep(0, nrow(dat)), time, status) ~  rx + sex + cluster(litter),
#'               .distribution = emfrail_distribution(dist = "stable"),
#'               .control = emfrail_control(verbose = FALSE))
#' m3
#'
#'
#'
#'
#' # draw the profile likelihood for different values of the frailty variance (gamma frailty)
#' fr_var <- c(0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)
#' prof_lik <- sapply(1/fr_var, function(th)
#'   -emfrail(.data =  dat, .formula = Surv(rep(0, nrow(dat)), time, status) ~  rx + sex + cluster(litter),
#'            .distribution = emfrail_distribution(frailtypar = th), .control = emfrail_control(opt_fit = FALSE))
#' )
#'
#' plot(fr_var, prof_lik, xlab = "frailty variance", ylab = "log-likelihood")
#'
#' # check with coxph:
#' prof_lik_cph <- sapply(fr_var, function(th)
#'   survival::coxph(data =  dat, formula = Surv(rep(0, nrow(dat)), time, status) ~ rx + sex + frailty(litter, theta = th),
#'                  method = "breslow")$history[[1]][[3]])
#'
#' lines(fr_var, prof_lik_cph, col = 2)
#'
#'
#' # simulated data ----------------------------------------------------------
#' set.seed(1)
#' x <- sample(c(0,1), 300, TRUE)
#' z <- rep(rgamma(100, 1, 1), each = 3)
#'
#' time <- rexp(300, rate = z * exp(0.5*x) )
#'
#' censtime <- 5
#' status <- rep(1, 300)
#'
#' status[time >= censtime] <- 0
#' time[status == 0] <- censtime
#' time0 <- rep(0, 300)
#'
#' dd <- data.frame(id = rep(1:100, each = 3), x = x, time0 = time0, time = time, status = status)
#'
#'
#' emfrail(.data =  dd, .formula = Surv(rep(0, nrow(dat)), time, status) ~  x + cluster(id),
#'         .distribution = emfrail_distribution( dist = "stable"),
#'         .control = emfrail_control(opt_fit = TRUE))
#'
#'
#' #Recurrent events --------------------------------------------------------
#'
#' data("bladder")
#' coxph(Surv(start, stop, status) ~ treatment + frailty(id), data = bladder1, method = "breslow")
#' mod2 <- emfrail(bladder1, Surv(start, stop, status) ~ treatment + cluster(id))
#'
#' # Plotting the baseline cumulative hazard / intensity
#'
#' with(mod2$res$haz,
#'      plot(time, cumhaz, type = "s", ylim = c(0, max(cumhaz) + 2 * max(se_chz_adj)),
#'      main = "Cumulative baseline hazard",
#'      ylab = "H0"))
#' with(mod2$res$haz, lines(time, cumhaz - 1.96*se_chz, col = 2))
#' with(mod2$res$haz, lines(time, cumhaz + 1.96*se_chz, col = 2))
#' with(mod2$res$haz, lines(time, cumhaz - 1.96*se_chz_adj, col = 3, lty = 2))
#' with(mod2$res$haz, lines(time, cumhaz + 1.96*se_chz_adj, col = 3, lty = 2))
#' legend(x = 0, y = 5, legend = c("95% CI", "adjusted 95% CI"), col = c(2,3), lty = c(1,2))
#'
#' # with ggplot2
#' library(ggplot2)
#' ggplot(mod2$res$haz, aes(x = time)) +
#' geom_ribbon(aes(ymin = cumhaz - 1.96*se_chz, ymax = cumhaz + 1.96*se_chz),  fill = "grey70") +
#'   geom_ribbon(aes(ymin = cumhaz - 1.96*se_chz_adj, ymax = cumhaz + 1.96*se_chz_adj),  fill = "pink", alpha = 0.2) +
#'   geom_step(aes(y = cumhaz)) +
#'   ggtitle("Cumulative baseline hazard") +
#'   ylab("H0")
#'
#'
#'  # Left truncation
#'  # simulate 300 clusters of size 5
#' set.seed(17)
#' x <- sample(c(0,1), 5 * 300, TRUE)
#' u <- rep(rgamma(300,1,1), each = 5)
#' stime <- rexp(5*300, rate = u * exp(x))
#' ltime <- runif(5 * 300)
#'
#' library(tidyverse)
#' d <- data.frame(id = rep(1:300, each = 5),
#'                 x = x,
#'                 stime = stime,
#'                 u = u,
#'                 ltime = ltime,
#'                 status = 1)
#'
#'
#' d1 <- d %>% filter(stime > ltime)
#'  # this is the same as the cph (naive):
#' mymod <- d1 %>%
#'   emfrail(Surv(ltime, stime, status)~ x + cluster(id), .control = emfrail_control(verbose = TRUE))
#'
#'  # this is the correct way here:
#' mymod <- d1 %>%
#'   emfrail(Surv(ltime, stime, status)~ x + cluster(id), .control = emfrail_control(verbose = TRUE),
#'           .distribution = emfrail_distribution(left_truncation = TRUE))


emfrail <- function(.data, .formula,
                    .distribution = emfrail_distribution(),
                    .control = emfrail_control()) {

  if(!inherits(.distribution, "emfrail_distribution"))
    stop(".distribution argument misspecified; see ?emfrail_distribution()")

  if(!inherits(.control, "emfrail_control"))
    stop(".control argument misspecified; see ?emfrail_control()")

  if(isTRUE(.control$fast_fit)) {
    if(!(.distribution$dist %in% c("gamma", "pvf"))) {
      #message("fast_fit option only available for gamma and pvf with m=-1/2 distributions")
      .control$fast_fit <- FALSE
    }
    if(.distribution$dist == "pvf" & (.distribution$pvfm != -1/2)) {
      #message("fast_fit option only available for gamma and pvf with m=-1/2 distributions")
      .control$fast_fit <- FALSE
    }

    if(.distribution$dist == "pvf" & (.distribution$pvfm == -1/2) & .distribution$left_truncation ) {
      #message("fast_fit option not available with left truncation for the Inverse Gaussian")
      .control$fast_fit <- FALSE
    }
  }


  Call <- match.call()


  if(missing(.formula)  | missing(.data)) stop("Missing arguments")
  # if(missing(.distribution)) {
  #   .distribution <-  emfrail_distribution()
  #   print("default distribution")
  # }
  # if(missing(.control)) .control <- emfrail_control()

  cluster <- function(x) x
  mf <- model.frame(.formula, .data)

  # Identify the cluster and the ID column
  pos_cluster <- grep("cluster", names(mf))
  if(length(pos_cluster) != 1) stop("misspecified or non-specified cluster")
  id <- mf[[pos_cluster]]

  # Identify Surv object
  Y <- mf[[1]]
  if(!inherits(Y, "Surv")) stop("left hand side not a survival object")
  if(attr(Y, "type") != "counting") stop("use Surv(tstart, tstop, status)")





  # get the model matrix
  X1 <- model.matrix(.formula, .data)
  # this is necessary because when factors have more levels, pos_cluster doesn't correspond any more
  pos_cluster_X1 <- grep("cluster", colnames(X1))
  X <- X1[,-c(1, pos_cluster_X1), drop=FALSE]
  # note: X has no attributes, in coxph it does.


  # some stuff for creating the C vector, is used all along.
  # mcox also works with empty matrices, but also with NULL as x.
  mcox <- survival::agreg.fit(x = X, y = Y, strata = NULL, offset = NULL, init = NULL,
                              control = survival::coxph.control(),
                              weights = NULL, method = "breslow", rownames = NULL)

  # the "baseline" case // this will stay constant

  if(length(X) == 0) {
    newrisk <- 1
    exp_g_x <- matrix(rep(1, length(mcox$linear.predictors)), nrow = 1)
    g <- 0
  } else {
    x2 <- matrix(rep(0, ncol(X)), nrow = 1, dimnames = list(123, dimnames(X)[[2]]))
    x2 <- (scale(x2, center = mcox$means, scale = FALSE))
    newrisk <- exp(c(x2 %*% mcox$coefficients) + 0)
    exp_g_x <- exp(mcox$coefficients %*% t(X))
    g <- mcox$coefficients
  }

  explp <- exp(mcox$linear.predictors) # these are with centered covariates

  # hh <- getchz(Y = Y, newrisk = newrisk, explp = explp)



  # cumhaz_line <- sapply(X = apply(as.matrix(Y[,c(1,2)]), 1, as.list),
  #                       FUN = function(x)  sum(hh$haz_tev[x$start <= hh$tev & hh$tev <= x$stop])) *
  #   exp_g_x # this is supposed to be without centered covariates.
  # #
  # #

  # #
  # basehaz_line <- hh$haz_tev[match(Y[,2], hh$tev)]

  # Cvec <- tapply(X = cumhaz_line,
  #                INDEX = id,
  #                FUN = sum)

  # the shorter way

  # new way of doing the things;

  nev_id <- rowsum(Y[,3], id) # nevent per id or am I going crazy

  Cvec <- nev_id - as.vector(rowsum(mcox$residuals, id))

  cumhaz_line <- (Y[,3] - mcox$residuals) # with covariates!



  # for the baseline hazard how the fuck is that gonna happen?
  # Idea: nrisk has the sum of elp who leave later at every tstop
  # esum has the sum of elp who enter at every tstart
  # indx groups which esum is right after each nrisk;
  # the difference between the two is the sum of elp really at risk at that time point.


  nrisk <- rev(cumsum(rev(rowsum(explp, Y[, ncol(Y) - 1]))))
  esum <- rev(cumsum(rev(rowsum(explp, Y[, 1]))))

  # the stuff that won't change
  death <- (Y[, ncol(Y)] == 1)
  nevent <- as.vector(rowsum(1 * death, Y[, ncol(Y) - 1])) # per time point
  time <- sort(unique(Y[,2])) # unique tstops

  # this gives the next entry time for each unique tstop (not only event)
  etime <- c(0, sort(unique(Y[, 1])))
  indx <- findInterval(time, etime)

  # this gives for every tstart (line variable) after which event time did it come
  indx2 <- findInterval(Y[,1], time, left.open = TRUE)
  time_to_stop <- match(Y[,2], time)
  order_id <- findInterval(id, unique(id))

  atrisk <- list(death = death, nevent = nevent, nev_id = nev_id,
                 order_id = order_id, time = time, indx = indx, indx2 = indx2,
                 time_to_stop = time_to_stop)

  nrisk <- nrisk - c(esum, 0)[indx]
  haz <- nevent/nrisk * newrisk

  basehaz_line <- haz[match(Y[,2], time)]

  #plot(haz[match(Y[,2], time)], basehaz_line)
  # cumhaz_full is like follows (from 0 to t)


  #
  # cumhaz_0_line <- cumhaz_tstop[match(Y[,2], time)]
  #
  # plot(Y[,2], cumhaz_0_line) # this should be increasing
  # # question is: for each tstart in the data we need to know which time point to fucking take for the cumulative hazard

  # cumhaz_line <- (cumhaz_0_line - cumhaz_tstart) * explp

  # cumulative hazard?


  #Cvec_from0 - Cvec

  if(isTRUE(.distribution$left_truncation)) {
    #indx2 <- findInterval(Y[,1], time, left.open = TRUE)
    cumhaz_tstop <- cumsum(haz)
    cumhaz_tstart <- c(0, cumhaz_tstop)[indx2 + 1]

    Cvec_lt <- rowsum(cumhaz_tstart, order_id)
    # Cvec_lt <- tapply(X = cumhaz_tstart,
    #                   INDEX = id,
    #                   FUN = sum)
  } else Cvec_lt <- 0 * Cvec


  # a fit just for the log-likelihood;
  if(!isTRUE(.control$opt_fit)) {
    return(em_fit(logfrailtypar = log(.distribution$frailtypar),
           dist = .distribution$dist, pvfm = .distribution$pvfm,
           Y = Y, Xmat = X, atrisk = atrisk, basehaz_line = basehaz_line,
           mcox = list(coefficients = g, loglik = mcox$loglik),  # a "fake" cox model
           Cvec = Cvec, lt = .distribution$left_truncation,
           Cvec_lt = Cvec_lt,
          .control = .control))
  }


  # otherwise, the maximizer
  opt_object <- optimx::optimx(par = log(.distribution$frailtypar), fn = em_fit,
               hessian = TRUE,
               #lower = -10000, upper = 10000,
               method = .control$opt_control$method, #control = .control$opt_control$control,
               #control = list(trace = 10, save.failures = TRUE),
               dist = .distribution$dist, pvfm = .distribution$pvfm,
               Y = Y, Xmat = X, atrisk = atrisk, basehaz_line = basehaz_line,
               mcox = list(coefficients = g, loglik = mcox$loglik),  # a "fake" cox model
               Cvec = Cvec, lt = .distribution$left_truncation,
               Cvec_lt = Cvec_lt,
               .control = .control)

  message("Calculating final fit with information matrix...")

  final_fit <- em_fit(logfrailtypar = opt_object$p1,
                      dist = .distribution$dist, pvfm = .distribution$pvfm,
                      Y = Y, Xmat = X, atrisk = atrisk, basehaz_line = basehaz_line,
                      mcox = list(coefficients = g, loglik = mcox$loglik),  # a "fake" cox model
                      Cvec = Cvec, lt = .distribution$left_truncation,
                      Cvec_lt = Cvec_lt,
                      .control = .control, return_loglik = FALSE)

  # that the hessian
  h <- as.numeric(sqrt(1/(attr(opt_object, "details")[[3]]))/2)
  lfp_minus <- max(opt_object$p1 - h , opt_object$p1 - 5)
  lfp_plus <- min(opt_object$p1 + h , opt_object$p1 + 5)

  message("Calculating adjustment for information matrix...")


  final_fit_minus <- em_fit(logfrailtypar = lfp_minus,
                            dist = .distribution$dist, pvfm = .distribution$pvfm,
                            Y = Y, Xmat = X, atrisk = atrisk, basehaz_line = basehaz_line,
                            mcox = list(coefficients = g, loglik = mcox$loglik),  # a "fake" cox model
                            Cvec = Cvec, lt = .distribution$left_truncation,
                            Cvec_lt = Cvec_lt,
                            .control = .control, return_loglik = FALSE)

  final_fit_plus <- em_fit(logfrailtypar = lfp_plus,
                           dist = .distribution$dist, pvfm = .distribution$pvfm,
                           Y = Y, Xmat = X, atrisk = atrisk, basehaz_line = basehaz_line,
                           mcox = list(coefficients = g, loglik = mcox$loglik),  # a "fake" cox model
                           Cvec = Cvec, lt = .distribution$left_truncation,
                           Cvec_lt = Cvec_lt,
                           .control = .control, return_loglik = FALSE)

  # instructional: this should be more or less equal to the
  # -(final_fit_plus$loglik + final_fit_minus$loglik - 2 * final_fit$loglik)/h^2

  # se_logtheta^2 / (2 * (final_fit$loglik -final_fit_plus$loglik ))

  deta_dtheta <- (c(final_fit_plus$coef, final_fit_plus$haz$haz_tev) -
    c(final_fit_minus$coef, final_fit_minus$haz$haz_tev)) / (2*h)

  #adj_se <- sqrt(diag(deta_dtheta %*% (1/(attr(opt_object, "details")[[3]])) %*% t(deta_dtheta)))


  ncoef = length(final_fit$coef)

  vcov_adj = final_fit$Vcov + deta_dtheta %*% (1/(attr(opt_object, "details")[[3]])) %*% t(deta_dtheta)

  varH <- final_fit$Vcov[(ncoef + 1): nrow(final_fit$Vcov), (ncoef+ 1): nrow(final_fit$Vcov)]
  varH_adj <- vcov_adj[(ncoef + 1): nrow(final_fit$Vcov), (ncoef+ 1): nrow(final_fit$Vcov)]

  varH_time <- numeric(nrow(varH))
  for(i in 1:nrow(varH)) {
    varH_time[i] = sum(varH[1:i, 1:i])
  }

  varH_adj_time <- numeric(nrow(varH))
  for(i in 1:nrow(varH)) {
    varH_adj_time[i] = sum(varH_adj[1:i, 1:i])
  }


res <- list(outer_m = opt_object,
              res = list(loglik = final_fit$loglik,
                         dist = final_fit$dist,
                         pvfm = final_fit$pvfm,
                         theta = final_fit$frailtypar,
                         haz = data.frame(time = final_fit$haz$tev,
                                         cumhaz = cumsum(final_fit$haz$haz_tev),
                                         se_chz = sqrt(varH_time),
                                         se_chz_adj = sqrt(varH_adj_time)),
                                          #se = final_fit$se[(1 + length(final_fit$coef)):length(final_fit$se)]),
                         z = data.frame(id = unique(id),
                                        Lambda = final_fit$Cvec,
                                        z = final_fit$estep[,1] / final_fit$estep[,2] ),
                                        coef = final_fit$coef,
                         se_coef = sqrt(diag(final_fit$Vcov)[seq_along(final_fit$coef)]),
                         se_coef_adj = sqrt(diag(vcov_adj)[seq_along(final_fit$coef)] )),
                   mcox = mcox,
              vcov = final_fit$Vcov,
              vcov_adj = vcov_adj
               )
  attr(res, "class") <- "emfrail"

  res


}
